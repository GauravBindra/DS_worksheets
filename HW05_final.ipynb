{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS 505 Homework 05:  Recurrent Neural Networks\n",
    "\n",
    "#### Due Monday  11/27 at midnight (1 minute after 11:59 pm) in Gradescope (with a grace period of 6 hours)\n",
    "#### You may submit the homework up to 24 hours late (with the same grace period) for a penalty of 10%. \n",
    "\n",
    "All homeworks will be scored with a maximum of 100 points; point values are given\n",
    "for individual problems, and if parts of problems do not have point values given, they\n",
    "will be counted equally toward the total for that problem. \n",
    "\n",
    "Note: This homework is a bit different from the first four in this class in that in some parts we are specified **what** you need to do for your solutions, but much less of the **how** you write the details of the code. There are three reasons for this:\n",
    "\n",
    "- In a graduate level CS class, after four homeworks and two months of lectures, you should be well-equipped to work out the coding issues for yourself, and in general, going forward, this is how you will solve the kinds of problems presented here; \n",
    "- Suggestions for resources (mostly ML blogs) will be suggested; there are many resources, but these are from bloggers that I trust and have used in the past;\n",
    "- I am expecting that you will make good use of chatGPT for help with the details of syntax and low-level organization of your code. There is often nothing very stimulating or informative about precisely what is the syntax needed for a particular kind of layer in a network, and rather than poke around on StackOverflow, chatGPT is particularly good at summarizing existing approaches to ML coding tasks. \n",
    "\n",
    "#### Submission Instructions\n",
    "\n",
    "You must complete the homework by editing <b>this notebook</b> and submitting the following two files in Gradescope by the due date and time:\n",
    "\n",
    "  - A file <code>HW05.ipynb</code> (be sure to select <code>Kernel -> Restart and Run All</code> before you submit, to make sure everything works); and\n",
    "  - A file <code>HW05.pdf</code> created from the previous.\n",
    "  \n",
    "  For best results obtaining a clean PDF file on the Mac, select <code>File -> Print Review</code> from the Jupyter window, then choose <code>File-> Print</code> in your browser and then <code>Save as PDF</code>.  Something  similar should be possible on a Windows machine -- just make sure it is readable and no cell contents have been cut off. Make it easy to grade!\n",
    "  \n",
    "The date and time of your submission is the last file you submitted, so if your IPYNB file is submitted on time, but your PDF is late, then your submission is late. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborators (5 pts)\n",
    "\n",
    "Describe briefly but precisely\n",
    "\n",
    "1. Any persons you discussed this homework with and the nature of the discussion;\n",
    "2. Any online resources you consulted and what information you got from those resources; and\n",
    "3. Any AI agents (such as chatGPT or CoPilot) or other applications you used to complete the homework, and the nature of the help you received. \n",
    "\n",
    "A few brief sentences is all that I am looking for here. \n",
    "\n",
    "Discussed the approach of Problem 1 - character level language model with another student in the class - Zach\n",
    "Consulted the blog by Karpathy and the code present there. used chatGPT for syntax \n",
    "\n",
    "For problem 2 i consulted prof's lecture on Youtube and lecture slides. \n",
    "also consulted the blog linked and used ChatGPT to help convert the code from keras to pytorch whereever i got stuck \n",
    "i had got stuck in beam-search, perplexity part of the problem and i asked zach for help there. \n",
    "\n",
    "For problem 3, i heavily consulted chatGPT specially for 3C and optional. \n",
    "The prof had mentioned on Piazza that this was encouraged. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "from numpy.random import shuffle, seed, choice\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import random_split,Dataset,DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torch import nn, optim\n",
    "\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "import string\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem One:  Character-Level Generative Model (20 pts)\n",
    "\n",
    "A basic character-level model has been provided on the class web site in the row for Lecture 14: \n",
    "<a href=\"https://www.cs.bu.edu/fac/snyder/cs505/CharacterLevelLSTM.ipynb\">IPYNB</a>. Your first step is to download this and run it in Colab (or download the data file, which is in the CS 505 Data Directory and also linked on the web site, and run it on your local machine) and understand all its various features. Most of it is straight-forward at this point in the course, but the definition of the model is a bit messy, and you will need to read about LSTM layers in the Pytorch documents to really understand what it is doing and what the hyperparameters mean. \n",
    "\n",
    "Also take a look at the article \"The Unreasonable Effectiveness of Recurrent Neural Networks\" linked with lecture 14. \n",
    "\n",
    "For this problem, you will run this code on a dataset consisting of Java code files, which has been uploaded to the CS 505 Data Directory and also to the class web site: <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/JavaFiles/\">DIR</a>  Select some number of these files and concatenate them into one long text file, such that you have approximately 10-20K characters (if you have trouble running out of RAM you can use fewer, but try to get at least 10K). \n",
    "\n",
    "You will run the character-level model on this dataset. You may either cut and paste code into this notebook, or submit the file with your changes and output along with this notebook to Gradescope.\n",
    "\n",
    "Your task is to get a character-level model that has not simply memorized the Java text file by overfitting, and does not do much other than spit out random characters (underfitting).  You will get the former if you simply run it for many epochs without any changes to the hyperparameters; you will get the latter if you run it only a few epochs. \n",
    "\n",
    "You should experiment with different hyperparameters, which in the notebook are indicated\n",
    "by \n",
    "\n",
    "          <== something to play with\n",
    "\n",
    "and try to get a model that seems to recognize typical Java syntax such as comments, matching parentheses, expressions, assignments, and formatting, but is not just repeating\n",
    "exact text from the data file. Clearly, the number of epochs plays a crucial role, but I also want you to\n",
    "experiment with the various hyperparameters to try to avoid overfitting. See my lectures on T 10/31 and Th 11/2 (recorded and on my YT channel) for the background to this.\n",
    "\n",
    "Note that the code you will work from does not use validation and testing sets, nor does it calculate the accuracy, but only tracks the loss. The nature of the data sets for character-level models does not seem to lend itself to accuracy metrics, but you may wish to try this -- I have not found it to be useful, but have simply focussed on the output and \"eyeballed\" the results to determine how much they have generalized\n",
    "from the data. \n",
    "\n",
    "Submit your notebook(s) to Gradescope as usual, and also provide a summary of your results in the next cell. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download and read the file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'public class HashTableLPResizing {\\n  \\n  private final int neverUsed = -1;\\n  private final int usedBut'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"HashTableLPResizing.java\", \"r\") as text_file:\n",
    "    text = text_file.read()\n",
    "\n",
    "text[:101]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Info about the file \n",
    "\n",
    "\n",
    "Training a character-level language model on the provided Java file, \"HashTableLPResizing.java,\" is an interesting project. This file appears to be a Java class implementation of a hash table with linear probing and dynamic resizing. The class includes various methods for inserting, deleting, checking membership, resizing the table, rehashing, and printing the table contents.\n",
    "\n",
    "Training a character-level model on this code can help the model learn Java syntax, programming patterns specific to hash table implementations, and even stylistic nuances of code writing, like commenting and structuring.\n",
    "\n",
    "The process involves feeding the model with the text of this Java file and allowing it to learn the patterns in the characters. The model will then be able to generate text that resembles Java code, potentially even synthesizing new code snippets that are syntactically correct and logically sound, based on the learned patterns.\n",
    "\n",
    "This approach can be useful for understanding code structure, assisting in code completion, and even in educational settings to help new programmers understand coding patterns and practices. Remember, the effectiveness of the model will largely depend on the size and diversity of the training data. Since this is a single file, the scope of learning will be limited to the patterns and structures within this specific implementation.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text is 10000 characters long.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Text is {len(text)} characters long.\")\n",
    "\n",
    "size = 10000\n",
    "\n",
    "text = text[:size]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "working with first 10k characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 75 characters in the text.\n",
      "Character set: ['\\n', ' ', '!', '\"', '%', '&', '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', 'A', 'B', 'E', 'F', 'H', 'K', 'L', 'M', 'N', 'O', 'P', 'R', 'S', 'T', 'U', 'W', '[', '\\\\', ']', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '}'].\n"
     ]
    }
   ],
   "source": [
    "chars_in_text = sorted(list(set(text)))\n",
    "num_chars = len(chars_in_text)\n",
    "print(f'There are {num_chars} characters in the text.')\n",
    "print(f'Character set: {chars_in_text}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create functions mapping characters to integers and back\n",
    "\n",
    "def char2int(c):\n",
    "    return chars_in_text.index(c)\n",
    "\n",
    "def int2char(i):\n",
    "    return chars_in_text[i] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_len = 75\n",
    "\n",
    "# Creating lists that will hold our input and target sample sequences\n",
    "\n",
    "input_seq_chars = []\n",
    "target_seq_chars = []\n",
    "\n",
    "for k in range(len(text)-sample_len+1):\n",
    "\n",
    "    # Remove last character for input sequence\n",
    "    input_seq_chars.append(text[k:k+sample_len-1])\n",
    "\n",
    "    # Remove firsts character for target sequence\n",
    "    target_seq_chars.append(text[k+1:k+sample_len])\n",
    "\n",
    "# for i in range(5):\n",
    "#     print(f'Input sequence:\\n{input_seq_chars[i]}')\n",
    "#     print(f'Target sequence:\\n{target_seq_chars[i]}')\n",
    "#     print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[63, 67, 49, 59, 56, 50, 1, 50, 59, 48, 65, 65, 1, 33, 48, 65, 55, 42, 48, 49, 59, 52, 35, 39, 40, 52, 65, 56, 72, 56, 61, 54, 1, 73, 0, 1, 1, 0, 1, 1, 63, 64, 56, 68, 48, 66, 52, 1, 53, 56, 61, 48, 59, 1, 56, 61, 66, 1, 61, 52, 68, 52, 64, 43, 65, 52, 51, 1, 27, 1, 11, 15, 25, 0]\n"
     ]
    }
   ],
   "source": [
    "input_seq = []\n",
    "target_seq = []\n",
    "\n",
    "for i in range(len(input_seq_chars)):\n",
    "    input_seq.append( [char2int(ch) for ch in input_seq_chars[i]])\n",
    "    target_seq.append([char2int(ch) for ch in target_seq_chars[i]])\n",
    "\n",
    "print(input_seq[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert an integer into a one-hot encoding of the given size (= number of characters)\n",
    "def int2OneHot(X,size):\n",
    "\n",
    "    def int2OneHot1(x,size=10):\n",
    "        tmp = np.zeros(size)\n",
    "        tmp[int(x)] = 1.0\n",
    "        return tmp\n",
    "\n",
    "    return np.array([ int2OneHot1(x, size) for x in X ]).astype('double')\n",
    "\n",
    "int2OneHot( np.array([ 2,3,1,2,3,4 ]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 1., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 1., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 1., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 1., 0., 0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# do the same thing, but for a list/array of integers\n",
    "\n",
    "def seq2OneHot(seq,size):\n",
    "    return np.array([ int2OneHot(x, size) for x in seq ])\n",
    "\n",
    "seq2OneHot( np.array([[ 2,3,1,2,3,4 ],[ 2,3,1,2,3,4 ],[ 2,3,1,2,3,4 ]]),10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9926, 74, 75)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert our input sequences to one-hot form\n",
    "\n",
    "input_seq = seq2OneHot(input_seq,size=num_chars)\n",
    "input_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9926, 74, 75)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert our target sequences to one-hot form\n",
    "\n",
    "target_seq = seq2OneHot(target_seq,size=num_chars)\n",
    "target_seq.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_seq = torch.Tensor(input_seq).type(torch.DoubleTensor)\n",
    "target_seq = torch.Tensor(target_seq).type(torch.DoubleTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9926"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class Basic_Dataset(Dataset):\n",
    "\n",
    "    def __init__(self, X,Y):\n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    # return a pair x,y at the index idx in the data set\n",
    "    def __getitem__(self, idx):\n",
    "        return self.X[idx], self.Y[idx]\n",
    "\n",
    "ds = Basic_Dataset(input_seq,target_seq)\n",
    "\n",
    "ds.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "\n",
    "data_loader = DataLoader(ds, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU not available, CPU used\n"
     ]
    }
   ],
   "source": [
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "# is_cuda = torch.cuda.is_available()\n",
    "is_cuda = False\n",
    "\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"GPU is available\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"GPU not available, CPU used\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import device_encoding\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_dim, n_layers,dropout):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        #Defining the layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_dim, n_layers,dropout=dropout,batch_first=True)\n",
    "        # Fully connected layer\n",
    "        self.fc1 = nn.Linear(hidden_dim, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        hidden_state_size = x.size(0)\n",
    "\n",
    "        x = x.to(torch.double)\n",
    "\n",
    "        h0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "        c0 = torch.zeros(self.n_layers,hidden_state_size,self.hidden_dim).double().to(device)\n",
    "\n",
    "        self.lstm = self.lstm.double()\n",
    "\n",
    "        self.fc1 = self.fc1.double()\n",
    "\n",
    "        # Passing in the input and hidden state into the model and obtaining outputs\n",
    "        out, (hx,cx) = self.lstm(x, (h0,c0))\n",
    "\n",
    "        # Reshaping the outputs such that it can be fit into the fully connected layer\n",
    "        out = out.contiguous().view(-1, self.hidden_dim)\n",
    "        out = self.fc1(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model(\n",
      "  (lstm): LSTM(75, 25, batch_first=True)\n",
      "  (fc1): Linear(in_features=25, out_features=75, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "\n",
    "model = Model(input_size=num_chars, output_size=num_chars, hidden_dim=25, n_layers=1,dropout=0)\n",
    "\n",
    "print(model)\n",
    "\n",
    "model = model.double().to(device)\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01,weight_decay=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 78/78 [00:08<00:00,  9.04it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 13.51it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 14.13it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 14.52it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 14.74it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.49it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.53it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.01it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.56it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.45it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.78it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.89it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.52it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.68it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.85it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.99it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.84it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.03it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.02it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.73it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.22it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.16it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.86it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.71it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.81it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.59it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.80it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.57it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.80it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.13it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.79it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.11it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.02it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.06it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.70it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.01it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.91it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.76it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 16.07it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.83it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.58it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.81it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.90it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.81it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 14.87it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.68it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.72it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.68it/s]\n",
      "100%|██████████| 78/78 [00:05<00:00, 15.57it/s]\n",
      "100%|██████████| 78/78 [00:04<00:00, 15.82it/s]\n",
      "100%|██████████| 50/50 [04:13<00:00,  5.08s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1965d40d0>]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "num_epochs = 50\n",
    "\n",
    "losses = []\n",
    "\n",
    "model.train()\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "\n",
    "    for input_seq_batch,target_seq_batch in tqdm(data_loader):\n",
    "        input_seq_batch = input_seq_batch.to(device)\n",
    "        target_seq_batch = target_seq_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        target_seq_hat = model(input_seq_batch)\n",
    "        loss = loss_fn(target_seq_hat,target_seq_batch.view(-1,num_chars))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    losses.append(loss.item())\n",
    "\n",
    "\n",
    "plt.title('Loss')\n",
    "plt.plot(losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example of softmax with temperature.\n",
      "distribution: [0.1, 0.3, 0.6]\n",
      "[1.9287498479637375e-22, 9.3576229688393e-14, 0.9999999999999064]\n",
      "[0.006377460922442302, 0.04712341652466416, 0.9464991225528936]\n",
      "[0.06289001324586753, 0.1709527801977903, 0.7661572065563421]\n",
      "[0.12132647558421489, 0.23631170657656433, 0.6423618178392208]\n",
      "[0.2583896517379799, 0.3155978333128144, 0.4260125149492058]\n",
      "[0.3255767455856355, 0.3321538321280155, 0.3422694222863489]\n"
     ]
    }
   ],
   "source": [
    "def softmax_with_temperature(vec, temperature):\n",
    "    sum_exp = sum(math.exp(x/temperature) for x in vec)\n",
    "    return [math.exp(x/temperature)/sum_exp for x in vec]\n",
    "\n",
    "print(\"Example of softmax with temperature.\")\n",
    "dist = [0.1, 0.3, 0.6]\n",
    "print('distribution:',dist)\n",
    "print(softmax_with_temperature(dist,0.01))\n",
    "print(softmax_with_temperature(dist,0.1))\n",
    "print(softmax_with_temperature(dist,0.2))\n",
    "print(softmax_with_temperature(dist,0.3))\n",
    "print(softmax_with_temperature(dist,1))\n",
    "print(softmax_with_temperature(dist,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temperature = 0.35\n",
    "\n",
    "def predict(model, ch):\n",
    "\n",
    "    # only look at last sample_len - 1 characters\n",
    "\n",
    "    ch = ch[-(sample_len - 1):]\n",
    "\n",
    "    # One-hot encoding our input to fit into the model\n",
    "    ch = np.array([char2int(c) for c in ch])\n",
    "    ch = np.array([int2OneHot(ch, num_chars)])\n",
    "    ch = torch.from_numpy(ch).to(device)\n",
    "\n",
    "    out = model(ch)\n",
    "\n",
    "    # take the probability distribution of the last character in the sequence produced by the model\n",
    "    prob = softmax_with_temperature(out[-1],temperature)\n",
    "\n",
    "    # Choosing a character based on the probability distribution, with temperature\n",
    "    char_ind = choice(list(range(num_chars)), p=prob)\n",
    "\n",
    "    return int2char(char_ind)\n",
    "\n",
    "#predict(model,\"Of man's first disobedience, and the fruit o\")\n",
    "predict(model,\"public class HashTableLPResiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample(model, out_len, start):\n",
    "    model.eval() # eval mode\n",
    "    # First off, run through the starting characters\n",
    "    chars = [ch for ch in start]\n",
    "    size = out_len - len(chars)\n",
    "    # Now pass in the previous characters and get a new one\n",
    "    for ii in range(size):\n",
    "        char = predict(model, chars)\n",
    "        chars.append(char)\n",
    "\n",
    "    return ''.join(chars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public class HashTableLPResizing();\n",
      "        A[i] = neverUsed;\n",
      "    if(inResizingMode) {                                               // n that or f return trig\n",
      "               // prime if(ANew[i] == neverUsed)\n",
      "          if(A[i] == key) {\n",
      "                     // next pric booloom] < 00) \n",
      "    for(i to ANew = -1;\n",
      "  private int i; \n",
      "    int N = 0.59 \n",
      "      , 797, 907, 979, 757, 737, 701, 701, 701, 709, 753, 887, 883, 887, 883, 881, 887, 887, 883, 871, 789, 781, 79, 87, 87, 87, 87, 17, 17, 17, 11, 17, 11, 131, 141, 151, 151, 163, 197, 199, 107, 109, 123,1129,1753,1189,1187,1753,1757,1781,1873,1873,1899,1889e\n",
      "       // key table in rus insert = \"+ \"+i+\"]: \"); \n",
      "      else if(ANew[i] == neverUsed;   return trig && relsizen(\"ANew[\"+i+\"]: \" + A[i]); \n",
      "    }\n",
      "        System.out.println(\"ANew[\"+i+\"]: \" + A[i]); \n",
      "    }\n",
      "      for(i = hash(key,A.length); A[i] != neverUsed && remaining > 0; i = (i + 1) % ANew.length) {\n",
      "        if(ANew[i] == neverUsed;   return trig\n",
      "        private int K = A.length; \n",
      "     \n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, \"public class HashTableLPRes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Result after 10 epochs \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "public class HashTableLPResizing(); "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Result after 25 epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- public class HashTableLPResizing();\n",
    "        A[i] = neverUsed;\n",
    "    if(inResizingMode) {                                               // n that or f return trig\n",
    "               // prime if(ANew[i] == neverUsed)\n",
    "          if(A[i] == key) {\n",
    "                     // next pric booloom] < 00) \n",
    "    for(i to ANew = -1;\n",
    "  private int i; \n",
    "    int N = 0.59 \n",
    "      , 797, 907, 979, 757, 737, 701, 701, 701, 709, 753, 887, 883, 887, 883, 881, 887, 887, 883, 871, 789, 781, 79, 87, 87, 87, 87, 17, 17, 17, 11, 17, 11, 131, 141, 151, 151, 163, 197, 199, 107, 109, 123,1129,1753,1189,1187,1753,1757,1781,1873,1873,1899,1889e\n",
    "       // key table in rus insert = \"+ \"+i+\"]: \"); \n",
    "      else if(ANew[i] == neverUsed;   return trig && relsizen(\"ANew[\"+i+\"]: \" + A[i]); \n",
    "    }\n",
    "        System.out.println(\"ANew[\"+i+\"]: \" + A[i]); \n",
    "    }\n",
    "      for(i = hash(key,A.length); A[i] != neverUsed && remaining > 0; i = (i + 1) % ANew.length) {\n",
    "        if(ANew[i] == neverUsed;   return trig\n",
    "        private int K = A.length;  -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:3\u001b[0;36m\u001b[0m\n\u001b[0;31m    if(inResizingMode) {                                               // n that or f return trig\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "public class HashTableLPResizing();\n",
    "        A[i] = neverUsed;\n",
    "    if(inResizingMode) {                                               // n that or f return trig\n",
    "               // prime if(ANew[i] == neverUsed)\n",
    "          if(A[i] == key) {\n",
    "                     // next pric booloom] < 00) \n",
    "    for(i to ANew = -1;\n",
    "  private int i; \n",
    "    int N = 0.59 \n",
    "      , 797, 907, 979, 757, 737, 701, 701, 701, 709, 753, 887, 883, 887, 883, 881, 887, 887, 883, 871, 789, 781, 79, 87, 87, 87, 87, 17, 17, 17, 11, 17, 11, 131, 141, 151, 151, 163, 197, 199, 107, 109, 123,1129,1753,1189,1187,1753,1757,1781,1873,1873,1899,1889e\n",
    "       // key table in rus insert = \"+ \"+i+\"]: \"); \n",
    "      else if(ANew[i] == neverUsed;   return trig && relsizen(\"ANew[\"+i+\"]: \" + A[i]); \n",
    "    }\n",
    "        System.out.println(\"ANew[\"+i+\"]: \" + A[i]); \n",
    "    }\n",
    "      for(i = hash(key,A.length); A[i] != neverUsed && remaining > 0; i = (i + 1) % ANew.length) {\n",
    "        if(ANew[i] == neverUsed;   return trig\n",
    "        private int K = A.length; "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output after 50 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public class HashTableLPResizing H = new HashTableLPResizing(); \n",
      "    H.printTables();\n",
      "     T.insert( {\n",
      "    thins giner into the slot isEmpt intauctor gis \n",
      "  \n",
      "  private int nextSlot = 0;          // pld tubl , e aretiach (Stric sea keyimetraty thesimple one on ner rehashing(\"+key+\"Ne\"\"\"); \n",
      "        System.out.println(\"A[\"+i+\"]: \" + A[i]); \n",
      "    }\n",
      "      if(p, N == 0);   \n",
      "    ,1909,1729,1701,1701,1707,1727,1753, 757, 763, 753, 761, 733 \n",
      "    , 737, 797, 701, 709, 723, 733, 739, 733 \n",
      "    , 73, 8, \n",
      "  p, 66, 67, 71, 87, 673, 619, 619, 643, 649, 643, 653, 2679, 643, 653, 649, 653, 649, 643, 649, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 607, 619, 643, 641, 643, 653, 2679, 653, 653, 653, 653, 601, 607, 619, 641, 641, 643, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 607, 643, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 2067, 2089, 2273, 2111, 2377, 2387, \n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, \"public class HashTableLPRes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated triple-quoted string literal (detected at line 13) (934255942.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[39], line 6\u001b[0;36m\u001b[0m\n\u001b[0;31m    private int nextSlot = 0;          // pld tubl , e aretiach (Stric sea keyimetraty thesimple one on ner rehashing(\"+key+\"Ne\"\"\");\u001b[0m\n\u001b[0m                                                                                                                               ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 13)\n"
     ]
    }
   ],
   "source": [
    "public class HashTableLPResizing H = new HashTableLPResizing(); \n",
    "    H.printTables();\n",
    "     T.insert( {\n",
    "    thins giner into the slot isEmpt intauctor gis \n",
    "  \n",
    "  private int nextSlot = 0;          // pld tubl , e aretiach (Stric sea keyimetraty thesimple one on ner rehashing(\"+key+\"Ne\"\"\"); \n",
    "        System.out.println(\"A[\"+i+\"]: \" + A[i]); \n",
    "    }\n",
    "      if(p, N == 0);   \n",
    "    ,1909,1729,1701,1701,1707,1727,1753, 757, 763, 753, 761, 733 \n",
    "    , 737, 797, 701, 709, 723, 733, 739, 733 \n",
    "    , 73, 8, \n",
    "  p, 66, 67, 71, 87, 673, 619, 619, 643, 649, 643, 653, 2679, 643, 653, 649, 653, 649, 643, 649, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 607, 619, 643, 641, 643, 653, 2679, 653, 653, 653, 653, 601, 607, 619, 641, 641, 643, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 607, 643, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 653, 601, 2067, 2089, 2273, 2111, 2377, 2387,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output after 100 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "public class HashTableLPResizing {\n",
      "      // move one key fullrger A = new latic booch the \n",
      "      }\n",
      "      // move one key from old table and return the on that into ANew\n",
      "  // note that duplicates may be inserted into A because of a deletion of a key higher resizing mode\n",
      "  // and arrayFac old table\n",
      "  \n",
      "  private void printTables() {\n",
      "    printTable(); \n",
      "    if(inResizingMode)\n",
      "      printTableNew();\n",
      "    System.out.println(\"N = \" + N + \"\\n\"); \n",
      "  }\n",
      "  \n",
      "  \n",
      "  private int N = 0;                                                          // numbert(i + i) \n",
      "    int i; \n",
      "    for(i = hash(key,A.length); A[i] != neverUsed && remaining > 0; i = (i + 1) % ANew.length) {\n",
      "        if(ANew[i] == key) {\n",
      "          if(ANew[i] == key) {\n",
      "          A[i] = usedButEmpty; \n",
      "        --N;\n",
      "        \n",
      "      }\n",
      "      if(ANew[i] == key) {\n",
      "          ANew[i] = usedButEmpty; \n",
      "          --N;\n",
      "        \n",
      "      }\n",
      "      if(ANew[i] == key) {\n",
      "          A[i] = usedButEmpty; \n",
      "        --N;\n",
      "        return NLPHelper(p,lo,mid-1);\n",
      "      else\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "print(sample(model, 1000, \"public class HashTableLPRes\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 5)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:5\u001b[0;36m\u001b[0m\n\u001b[0;31m    // note that duplicates may be inserted into A because of a deletion of a key higher resizing mode\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "public class HashTableLPResizing {\n",
    "      // move one key fullrger A = new latic booch the \n",
    "      }\n",
    "      // move one key from old table and return the on that into ANew\n",
    "  // note that duplicates may be inserted into A because of a deletion of a key higher resizing mode\n",
    "  // and arrayFac old table\n",
    "  \n",
    "  private void printTables() {\n",
    "    printTable(); \n",
    "    if(inResizingMode)\n",
    "      printTableNew();\n",
    "    System.out.println(\"N = \" + N + \"\\n\"); \n",
    "  }\n",
    "  \n",
    "  \n",
    "  private int N = 0;                                                          // numbert(i + i) \n",
    "    int i; \n",
    "    for(i = hash(key,A.length); A[i] != neverUsed && remaining > 0; i = (i + 1) % ANew.length) {\n",
    "        if(ANew[i] == key) {\n",
    "          if(ANew[i] == key) {\n",
    "          A[i] = usedButEmpty; \n",
    "        --N;\n",
    "        \n",
    "      }\n",
    "      if(ANew[i] == key) {\n",
    "...\n",
    "        --N;\n",
    "        return NLPHelper(p,lo,mid-1);\n",
    "      else"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we can see that even after 100 epochs the model cant produce error free code "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code Outline : \n",
    "\n",
    "Text Preprocessing: The script starts by truncating the input text to a fixed size and creates mappings (char2int, int2char) to convert characters to integers and vice versa.\n",
    "\n",
    "Sequence Creation: It generates input and target sequences for training, where each input sequence is mapped to a target sequence offset by one character.\n",
    "\n",
    "One-Hot Encoding: The sequences are converted into one-hot encoded format, which is a standard way to represent categorical data for neural networks.\n",
    "\n",
    "Dataset and DataLoader: A custom dataset class Basic_Dataset is defined for the input and target sequences, and a DataLoader is used for batching and shuffling the data.\n",
    "\n",
    "Model Definition: The Model class defines an LSTM-based neural network with a fully connected layer for output.\n",
    "\n",
    "Training Loop: The model is trained over a number of epochs, using cross-entropy loss and Adam optimizer.\n",
    "\n",
    "Temperature-based Sampling: A softmax function with temperature is used to generate a probability distribution for the next character, which adds variability to the generated text.\n",
    "\n",
    "Text Generation: The sample function generates text by repeatedly predicting the next character based on the previously generated characters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various hyper-parameters in the code \n",
    "\n",
    "size: The size of the text used for training, truncating the original text to this length.\n",
    "\n",
    "sample_len: Length of each sample sequence used for training.\n",
    "\n",
    "batch_size: Number of samples per batch in training.\n",
    "\n",
    "num_epochs: The number of training epochs, or full passes through the dataset.\n",
    "\n",
    "hidden_dim: The dimensionality of the hidden state in the LSTM.\n",
    "\n",
    "n_layers: The number of layers in the LSTM.\n",
    "\n",
    "dropout: The dropout rate for regularization in the LSTM.\n",
    "\n",
    "lr (learning rate): The learning rate for the Adam optimizer.\n",
    "\n",
    "weight_decay: The weight decay (L2 penalty) parameter for the optimizer.\n",
    "\n",
    "temperature: Used in temperature-based sampling for text generation, affecting the randomness of the predictions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Values chosen for various hyper-paramters \n",
    "\n",
    "<!-- size: 10000 (used to truncate the text for training)\n",
    "sample_len: 75 (length of each training sample sequence)\n",
    "batch_size: 128 (number of samples per batch in training)\n",
    "num_epochs: 50 (number of training epochs)\n",
    "hidden_dim: 25 (dimensionality of the hidden state in the LSTM)\n",
    "n_layers: 1 (number of LSTM layers)\n",
    "dropout: 0 (dropout rate for regularization in the LSTM)\n",
    "lr (learning rate): 0.01 (learning rate for the Adam optimizer)\n",
    "weight_decay: 0 (weight decay or L2 penalty for the optimizer)\n",
    "temperature: 0.35 (used in temperature-based sampling for text generation) -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values chosen for various hyper-parameters \n",
    "# size: 10000 (used to truncate the text for training)\n",
    "# sample_len: 75 (length of each training sample sequence)\n",
    "# batch_size: 128 (number of samples per batch in training)\n",
    "# num_epochs: 50 (number of training epochs)\n",
    "# hidden_dim: 25 (dimensionality of the hidden state in the LSTM)\n",
    "# n_layers: 1 (number of LSTM layers)\n",
    "# dropout: 0 (dropout rate for regularization in the LSTM)\n",
    "# lr (learning rate): 0.01 (learning rate for the Adam optimizer)\n",
    "# weight_decay: 0 (weight decay or L2 penalty for the optimizer)\n",
    "# temperature: 0.35 (used in temperature-based sampling for text generation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It would have been preferable to be able to use a bigger 'size' \n",
    "\n",
    "sample_len of 75 is good enough \n",
    "\n",
    "for batch_size, i tried 128 and 64. Could have tried 32 or lower if i had more time. \n",
    "\n",
    "num_epochs : tried till 100 epochs. Based on the graph, it doesnt seem like training further will help. \n",
    "\n",
    "hidden_dim : This might be low for complex tasks. Increasing it can capture more complex patterns but increases computational cost.\n",
    "\n",
    "n_layers : More layers can model more complex patterns, but also increase the risk of overfitting and computational cost.\n",
    "\n",
    "dropout : didnt have the time to experiment with this \n",
    "\n",
    "lr : standard values \n",
    "\n",
    "temperature : tried various values in the range .3 -1 : this worked well "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Your analysis\n",
    "\n",
    "Please describe your experiments and cut and paste various outputs to show how the model performed at\n",
    "various numbers of epochs and with various hyperparameters. What characteristics of Java was it able to learn? What did it not learn? The article \"The Unreasonable ...\" does a nice job of showing this kind of behavior as the number of epochs increases, and you might look at it before writing your answer here. \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Done above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Two:  Word-Level Generative Model (40 pts)\n",
    "\n",
    "In this problem you will write another generative model, as you did in HW 03, but this time you will use an LSTM network, GloVe word embeddings, and beam search. \n",
    "\n",
    "Before you start, read the following blog post to see the core ideas involved in creating a generative model using word embeddings:\n",
    "\n",
    "https://machinelearningmastery.com/how-to-develop-a-word-level-neural-language-model-in-keras/\n",
    "\n",
    "You may also wish to consult with chatGPT about how to develop this kind of model in Pytorch.\n",
    "\n",
    "The requirements for this problem are as follows (they mostly consist of the extensions proposed at\n",
    "the end of the blog post linked above):\n",
    "\n",
    "- Develop your code in Pytorch, not Keras\n",
    "- Use the novel *Persuation* by Jane Austen as your training data (available through the NLTK, you can just grab the sentences using `nltk.corpus.gutenberg.sents('austen-persuasion.txt')`); if you have trouble with RAM you will need to cut down the number of sentences (perhaps by eliminating the longest sentences as well, see next point). \n",
    "- Develop a sentence-level model by padding sentences to the maximum sentence length in the novel (if this seems extreme, you may wish to delete a small number of the longest sentences to reduce the maximum length). Surround your data sentences with `<s>` and `</s>` and your model should generate one sentence at a time (as you did in HW 03), i.e., it should stop if it generates the `</s>` token. \n",
    "- Use pretrained GLoVe embeddings with dimension 200, and update them (refine by training further) on the sentences in the novel; if you have trouble with RAM you may use a smaller dimension. \n",
    "- Experiment with the hyperparameters (sample length, number of layers, uni- or bi-directional, weight_decay, dropout, number of epochs, temperature of the softmax, etc.) as you did in Problem One to find the \"sweet spot\" where you are generating interesting-looking sentences but not simply repeating sentences from the data. You may want to try adding more linear layers on top to pick the most likely next word. \n",
    "- Generate sentences using Beam Search, which we describe below. \n",
    "\n",
    "Your solution should be the code, samples of sentences generated with their score (described below), and your description of the investigation of various hyperparameters, and what strategy ended up seeming to generate the most realistic sentences that were not simply a repeat of sentences in the data. \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Beam Search\n",
    "\n",
    "Beam search was described, and example shown, in Lecture 14. Here is a brief pseudo-code explaination of what\n",
    "you need to do:\n",
    "\n",
    "1. Develop your code as described above so that it can generate single sentences;\n",
    "2. Copy enough of your code over from HW 03 so that you can calculate the perplexity of\n",
    "        sentences (using the entire novel, or perhaps even a number of Jane Austen's novels as\n",
    "        the data source). As an alternative, you may wish to do this separately, store the nested dictionary\n",
    "        using Pickle, and load it here. \n",
    "3. Calculate the probability distribution of sentences in your data source that you used in the previous step, similar to what you did at the end of HW 01. \n",
    "4. Create a \"goodness function\" which estimates the quality of a sentence as the perplexity times the probability of its length.  This will be applied to all sequences of words, and not just sentences, but as a first approximation this is a way to attempt to make the distribution of sentence lengths similar to that in the novel.\n",
    "5. Follow the description in slide 7 of Lecture 14 to generate until you have 10 finished sentences. Print these out with their perplexity, probability of their length, and the combined goodness metric. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Code here \n",
    "import nltk \n",
    "nltk.download('gutenberg')\n",
    "from nltk.corpus import gutenberg\n",
    "sentences = gutenberg.sents('austen-persuasion.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ']']\n"
     ]
    }
   ],
   "source": [
    "print(sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ']']\n",
      "['Chapter', '1']\n",
      "['Sir', 'Walter', 'Elliot', ',', 'of', 'Kellynch', 'Hall', ',', 'in', 'Somersetshire', ',', 'was', 'a', 'man', 'who', ',', 'for', 'his', 'own', 'amusement', ',', 'never', 'took', 'up', 'any', 'book', 'but', 'the', 'Baronetage', ';', 'there', 'he', 'found', 'occupation', 'for', 'an', 'idle', 'hour', ',', 'and', 'consolation', 'in', 'a', 'distressed', 'one', ';', 'there', 'his', 'faculties', 'were', 'roused', 'into', 'admiration', 'and', 'respect', ',', 'by', 'contemplating', 'the', 'limited', 'remnant', 'of', 'the', 'earliest', 'patents', ';', 'there', 'any', 'unwelcome', 'sensations', ',', 'arising', 'from', 'domestic', 'affairs', 'changed', 'naturally', 'into', 'pity', 'and', 'contempt', 'as', 'he', 'turned', 'over', 'the', 'almost', 'endless', 'creations', 'of', 'the', 'last', 'century', ';', 'and', 'there', ',', 'if', 'every', 'other', 'leaf', 'were', 'powerless', ',', 'he', 'could', 'read', 'his', 'own', 'history', 'with', 'an', 'interest', 'which', 'never', 'failed', '.']\n",
      "['This', 'was', 'the', 'page', 'at', 'which', 'the', 'favourite', 'volume', 'always', 'opened', ':']\n",
      "['\"', 'ELLIOT', 'OF', 'KELLYNCH', 'HALL', '.']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sentence in sentences: \n",
    "    print(sentence)\n",
    "    count+=1\n",
    "    if count==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "for sentence in sentences: \n",
    "    sentence_len = len(sentence)\n",
    "    if sentence_len > max_len:\n",
    "        max_len = sentence_len\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "217"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The max length is 217"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3747"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_with_spaces = [word.replace('–', ' ') for word in sentence]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def clean_sentence(sentence):\n",
    "#     # Replace dashes with a white space\n",
    "#     sentence_with_spaces = [word.replace('–', ' ') for word in sentence]\n",
    "\n",
    "#     # Further cleaning: lowercasing and removing punctuation\n",
    "#     cleaned = [word.lower().strip(string.punctuation) for word in sentence_with_spaces if word.isalpha()]\n",
    "#     return cleaned\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "earlier had it removing punctuations as well, but after seeing the sentences generated, realised that it wasnt needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Replace dashes with a white space\n",
    "    sentence_with_spaces = [word.replace('–', ' ') for word in sentence]\n",
    "\n",
    "    # Further cleaning: lowercasing but keeping punctuation\n",
    "    cleaned = [word.lower() for word in sentence_with_spaces if word.isalpha() or word in string.punctuation]\n",
    "    return cleaned"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_sentences = [clean_sentence(sentence) for sentence in sentences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sir', 'walter', 'elliot', ',', 'of', 'kellynch', 'hall', ',', 'in', 'somersetshire', ',', 'was', 'a', 'man', 'who', ',', 'for', 'his', 'own', 'amusement', ',', 'never', 'took', 'up', 'any', 'book', 'but', 'the', 'baronetage', ';', 'there', 'he', 'found', 'occupation', 'for', 'an', 'idle', 'hour', ',', 'and', 'consolation', 'in', 'a', 'distressed', 'one', ';', 'there', 'his', 'faculties', 'were', 'roused', 'into', 'admiration', 'and', 'respect', ',', 'by', 'contemplating', 'the', 'limited', 'remnant', 'of', 'the', 'earliest', 'patents', ';', 'there', 'any', 'unwelcome', 'sensations', ',', 'arising', 'from', 'domestic', 'affairs', 'changed', 'naturally', 'into', 'pity', 'and', 'contempt', 'as', 'he', 'turned', 'over', 'the', 'almost', 'endless', 'creations', 'of', 'the', 'last', 'century', ';', 'and', 'there', ',', 'if', 'every', 'other', 'leaf', 'were', 'powerless', ',', 'he', 'could', 'read', 'his', 'own', 'history', 'with', 'an', 'interest', 'which', 'never', 'failed', '.']\n",
      "['Sir', 'Walter', 'Elliot', ',', 'of', 'Kellynch', 'Hall', ',', 'in', 'Somersetshire', ',', 'was', 'a', 'man', 'who', ',', 'for', 'his', 'own', 'amusement', ',', 'never', 'took', 'up', 'any', 'book', 'but', 'the', 'Baronetage', ';', 'there', 'he', 'found', 'occupation', 'for', 'an', 'idle', 'hour', ',', 'and', 'consolation', 'in', 'a', 'distressed', 'one', ';', 'there', 'his', 'faculties', 'were', 'roused', 'into', 'admiration', 'and', 'respect', ',', 'by', 'contemplating', 'the', 'limited', 'remnant', 'of', 'the', 'earliest', 'patents', ';', 'there', 'any', 'unwelcome', 'sensations', ',', 'arising', 'from', 'domestic', 'affairs', 'changed', 'naturally', 'into', 'pity', 'and', 'contempt', 'as', 'he', 'turned', 'over', 'the', 'almost', 'endless', 'creations', 'of', 'the', 'last', 'century', ';', 'and', 'there', ',', 'if', 'every', 'other', 'leaf', 'were', 'powerless', ',', 'he', 'could', 'read', 'his', 'own', 'history', 'with', 'an', 'interest', 'which', 'never', 'failed', '.']\n"
     ]
    }
   ],
   "source": [
    "print(cleaned_sentences[2])\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in cleaned_sentences]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_lengths, bins=range(1, max(sentence_lengths) + 1), edgecolor='black')\n",
    "plt.title(\"Histogram of Sentence Lengths in 'Persuasion'\")\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['[', 'Persuasion', 'by', 'Jane', 'Austen', '1818', ']']\n",
      "['Chapter', '1']\n",
      "['Sir', 'Walter', 'Elliot', ',', 'of', 'Kellynch', 'Hall', ',', 'in', 'Somersetshire', ',', 'was', 'a', 'man', 'who', ',', 'for', 'his', 'own', 'amusement', ',', 'never', 'took', 'up', 'any', 'book', 'but', 'the', 'Baronetage', ';', 'there', 'he', 'found', 'occupation', 'for', 'an', 'idle', 'hour', ',', 'and', 'consolation', 'in', 'a', 'distressed', 'one', ';', 'there', 'his', 'faculties', 'were', 'roused', 'into', 'admiration', 'and', 'respect', ',', 'by', 'contemplating', 'the', 'limited', 'remnant', 'of', 'the', 'earliest', 'patents', ';', 'there', 'any', 'unwelcome', 'sensations', ',', 'arising', 'from', 'domestic', 'affairs', 'changed', 'naturally', 'into', 'pity', 'and', 'contempt', 'as', 'he', 'turned', 'over', 'the', 'almost', 'endless', 'creations', 'of', 'the', 'last', 'century', ';', 'and', 'there', ',', 'if', 'every', 'other', 'leaf', 'were', 'powerless', ',', 'he', 'could', 'read', 'his', 'own', 'history', 'with', 'an', 'interest', 'which', 'never', 'failed', '.']\n",
      "['This', 'was', 'the', 'page', 'at', 'which', 'the', 'favourite', 'volume', 'always', 'opened', ':']\n",
      "['\"', 'ELLIOT', 'OF', 'KELLYNCH', 'HALL', '.']\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "for sentence in sentences: \n",
    "    print(sentence)\n",
    "    count+=1\n",
    "    if count==5:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3747"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cleaned_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = 0\n",
    "for sentence in cleaned_sentences: \n",
    "    sentence_len = len(sentence)\n",
    "    if sentence_len > 55:\n",
    "        count+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "392"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15585801974913263"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "392/3747"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Had this at 45 when pundctuations were not there. Changed to incorporate them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_sentences = [sentence for sentence in cleaned_sentences if len(sentence) <= 55]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sentence_lengths = [len(sentence) for sentence in filtered_sentences]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(sentence_lengths, bins=range(1, max(sentence_lengths) + 1), edgecolor='black')\n",
    "plt.title(\"Histogram of Sentence Lengths in 'Persuasion'\")\n",
    "plt.xlabel('Sentence Length')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 450/3747"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the padding token\n",
    "pad_token = \"<PAD>\"\n",
    "start_token = \"<s>\"\n",
    "end_token = \"</s>\"\n",
    "unknown_token = \"<UNK>\"\n",
    "\n",
    "# Pad each sentence to a length of 55\n",
    "padded_sentences = [[start_token] + sentence + [end_token] + [pad_token] * (55 - len(sentence)) \n",
    "    for sentence in filtered_sentences]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>', 'this', 'was', 'the', 'page', 'at', 'which', 'the', 'favourite', 'volume', 'always', 'opened', ':', '</s>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>', '<PAD>']\n"
     ]
    }
   ],
   "source": [
    "# Example: Check the first padded sentence\n",
    "print(padded_sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 4652\n"
     ]
    }
   ],
   "source": [
    "# Initialize a dictionary for the vocabulary\n",
    "vocabulary = {\n",
    "    pad_token: 0,\n",
    "    start_token: 1,\n",
    "    end_token: 2,\n",
    "    unknown_token :3\n",
    "}\n",
    "\n",
    "# Starting index for actual words\n",
    "word_index = 4\n",
    "\n",
    "# Loop through each sentence and add each unique word to the vocabulary\n",
    "for sentence in padded_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in vocabulary:\n",
    "            vocabulary[word] = word_index\n",
    "            word_index += 1\n",
    "\n",
    "# Now, 'vocabulary' is a dictionary mapping words to their integer index\n",
    "print(\"Vocabulary size:\", len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabulary['page']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "designed to convert each sentence in sentences into a list of integers based on the vocabulary dictionary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentences(sentences, vocabulary):\n",
    "    vectorized_data = []\n",
    "    unk_index = vocabulary.get(\"<UNK>\")  # Index for unknown words\n",
    "\n",
    "    for sentence in sentences:\n",
    "        vectorized_sentence = [vocabulary.get(word, unk_index) for word in sentence]\n",
    "        vectorized_data.append(vectorized_sentence)\n",
    "\n",
    "    return vectorized_data\n",
    "\n",
    "# Vectorizing your sentences\n",
    "vectorized_sentences = vectorize_sentences(padded_sentences, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vectorized_sentences[2])\n",
    "# vectorized_sentences[2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "57"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(padded_sentences[2])\n",
    "# padded_sentences[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use glove embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "converts a GloVe (Global Vectors for Word Representation) model file into a Word2Vec format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3f/x95cmnkn57s5pfth5lygr5k00000gn/T/ipykernel_81711/3076318043.py:4: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(glove_input_file, word2vec_output_file)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(400000, 200)"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "glove_input_file ='/Users/gauravbindra/desktop/NLP/hw05/glove.6B.200d.txt'\n",
    "word2vec_output_file = 'glove.6B.200d.txt.word2vec'\n",
    "glove2word2vec(glove_input_file, word2vec_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.6978678107261658)]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "# load the Stanford GloVe model\n",
    "filename = 'glove.6B.200d.txt.word2vec'\n",
    "model = KeyedVectors.load_word2vec_format(filename, binary=False)\n",
    "# calculate: (king - man) + woman = ?\n",
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "creates an embedding matrix that maps your vocabulary to the GloVe embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the embedding matrix \n",
    "\n",
    "import numpy as np\n",
    "\n",
    "embedding_dim = 200  # Dimension of GloVe vectors\n",
    "vocab_size = len(vocabulary)  # Your vocabulary size\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "\n",
    "for word, idx in vocabulary.items():\n",
    "    if word in model:\n",
    "        # Add the GloVe word embedding to the matrix\n",
    "        embedding_matrix[idx] = model[word]\n",
    "    else:\n",
    "        # Initialize with random or zero vector for words not found\n",
    "        embedding_matrix[idx] = np.random.randn(embedding_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4652"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embedding_matrix)\n",
    "# 4538 X 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMLanguageModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, embedding_matrix,num_layers=2):\n",
    "        super(LSTMLanguageModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.embedding.weight = nn.Parameter(torch.tensor(embedding_matrix, dtype=torch.float32))\n",
    "        self.embedding.weight.requires_grad = False  # Optionally freeze the embeddings\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, num_layers=2, batch_first=True, dropout=0.2)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        if hidden is None:\n",
    "            hidden = self.init_hidden(x.size(0))\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, hidden = self.lstm(embedded, hidden)\n",
    "        out = self.fc(lstm_out)\n",
    "\n",
    "        return out, hidden\n",
    "\n",
    "    def init_hidden(self, batch_size):\n",
    "        weight = next(self.parameters()).data\n",
    "        hidden = (weight.new(self.num_layers, batch_size, self.hidden_dim).zero_(),\n",
    "                  weight.new(self.num_layers, batch_size, self.hidden_dim).zero_())\n",
    "        return hidden\n",
    "\n",
    "# Parameters\n",
    "vocab_size = len(vocabulary)  # Size of your vocabulary\n",
    "embedding_dim = 200  # Dimension of GloVe embeddings\n",
    "hidden_dim = 256  # Number of features in the hidden state of the LSTM\n",
    "\n",
    "# Model Initialization\n",
    "model = LSTMLanguageModel(vocab_size, embedding_dim, hidden_dim, embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Assuming 'model' is the instance of your model class\n",
    "pad_index=0\n",
    "# Loss Function\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "\n",
    "class TextDataset(Dataset):\n",
    "    def __init__(self, vectorized_sentences):\n",
    "        self.data = vectorized_sentences\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = torch.tensor(self.data[idx][:-1], dtype=torch.long)  # Input sequence (all but last word)\n",
    "        y = torch.tensor(self.data[idx][1:], dtype=torch.long)   # Target sequence (all but first word)\n",
    "        return x, y\n",
    "\n",
    "# Assuming 'vectorized_sentences' is your list of vectorized and padded sentences\n",
    "dataset = TextDataset(vectorized_sentences)\n",
    "\n",
    "# Splitting the dataset\n",
    "train_size = int(0.7 * len(dataset))\n",
    "val_size = (len(dataset) - train_size) // 2\n",
    "test_size = len(dataset) - train_size - val_size\n",
    "train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size, test_size])\n",
    "\n",
    "# Creating DataLoaders\n",
    "batch_size = 64  # Adjust as needed\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "\n",
    "# num_epochs = 10  # Number of training epochs\n",
    "\n",
    "# for epoch in tqdm(range(num_epochs)):\n",
    "#     model.train()  # Set model to training mode\n",
    "#     hidden = model.init_hidden(batch_size)  # Initialize hidden state\n",
    "\n",
    "#     for inputs, targets in train_loader:\n",
    "#         hidden = tuple([each.data for each in hidden])  # Detach hidden state\n",
    "\n",
    "#         optimizer.zero_grad()  # Zero the gradients\n",
    "#         outputs, hidden = model(inputs, hidden)  # Forward pass\n",
    "#         loss = criterion(outputs.transpose(1, 2), targets)  # Compute loss\n",
    "#         loss.backward()  # Backpropagation\n",
    "#         optimizer.step()  # Update weights\n",
    "\n",
    "#     # Validation loop\n",
    "#     model.eval()  # Set model to evaluation mode\n",
    "#     with torch.no_grad():\n",
    "#         val_hidden = model.init_hidden(batch_size)\n",
    "#         val_loss = 0\n",
    "#         for inputs, targets in val_loader:\n",
    "#             val_hidden = tuple([each.data for each in val_hidden])\n",
    "#             outputs, val_hidden = model(inputs, val_hidden)\n",
    "#             val_loss += criterion(outputs.transpose(1, 2), targets).item()\n",
    "\n",
    "#     print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {loss.item()}, Validation Loss: {val_loss/len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 1/30 [00:37<18:20, 37.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30, Training Loss: 6.611564004743421, Validation Loss: 6.040038049221039\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 2/30 [01:12<16:40, 35.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/30, Training Loss: 5.96250429668942, Validation Loss: 6.004546344280243\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 3/30 [01:49<16:23, 36.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/30, Training Loss: 5.910895592457539, Validation Loss: 5.9863440990448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 4/30 [02:23<15:27, 35.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/30, Training Loss: 5.878444207681192, Validation Loss: 5.978936672210693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 5/30 [02:57<14:29, 34.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/30, Training Loss: 5.860545416136046, Validation Loss: 5.969632208347321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 6/30 [03:30<13:42, 34.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/30, Training Loss: 5.842778811583647, Validation Loss: 5.965262949466705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 23%|██▎       | 7/30 [04:01<12:45, 33.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/30, Training Loss: 5.80839577236691, Validation Loss: 5.9088321924209595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 27%|██▋       | 8/30 [04:33<12:00, 32.73s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/30, Training Loss: 5.736797242551236, Validation Loss: 5.8539857268333435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 9/30 [05:09<11:48, 33.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/30, Training Loss: 5.668054387376115, Validation Loss: 5.800485193729401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 10/30 [05:43<11:16, 33.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/30, Training Loss: 5.5935151125933675, Validation Loss: 5.7146897315979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 37%|███▋      | 11/30 [06:16<10:39, 33.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/30, Training Loss: 5.497283883996912, Validation Loss: 5.619724094867706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 12/30 [06:51<10:12, 34.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/30, Training Loss: 5.399263884570147, Validation Loss: 5.539345145225525\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 43%|████▎     | 13/30 [07:20<09:11, 32.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/30, Training Loss: 5.312443604340425, Validation Loss: 5.461858570575714\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 14/30 [07:50<08:28, 31.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/30, Training Loss: 5.225260657233161, Validation Loss: 5.392232835292816\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 15/30 [08:20<07:47, 31.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/30, Training Loss: 5.150775277936781, Validation Loss: 5.331539571285248\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 53%|█████▎    | 16/30 [08:49<07:09, 30.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/30, Training Loss: 5.076616441881335, Validation Loss: 5.277029871940613\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 57%|█████▋    | 17/30 [09:19<06:35, 30.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/30, Training Loss: 5.009884537877263, Validation Loss: 5.23568320274353\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 18/30 [09:49<06:04, 30.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/30, Training Loss: 4.951681833009462, Validation Loss: 5.203372895717621\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 63%|██████▎   | 19/30 [10:23<05:45, 31.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/30, Training Loss: 4.89495580260818, Validation Loss: 5.170692622661591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 20/30 [10:53<05:11, 31.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/30, Training Loss: 4.844646041457717, Validation Loss: 5.138916254043579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 21/30 [11:31<04:56, 32.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/30, Training Loss: 4.798003789540884, Validation Loss: 5.113717794418335\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 73%|███████▎  | 22/30 [22:46<30:06, 225.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/30, Training Loss: 4.749488985216296, Validation Loss: 5.090490818023682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 77%|███████▋  | 23/30 [23:19<19:35, 167.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/30, Training Loss: 4.705512201463854, Validation Loss: 5.071631073951721\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 24/30 [23:51<12:41, 126.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/30, Training Loss: 4.6623967918189795, Validation Loss: 5.05565071105957\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 83%|████████▎ | 25/30 [24:22<08:11, 98.25s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/30, Training Loss: 4.623549577352163, Validation Loss: 5.037358045578003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 87%|████████▋ | 26/30 [36:30<19:08, 287.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/30, Training Loss: 4.582103355510815, Validation Loss: 5.015509188175201\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 27/30 [37:15<10:43, 214.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/30, Training Loss: 4.544340816704002, Validation Loss: 5.002393543720245\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 93%|█████████▎| 28/30 [37:48<05:20, 160.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/30, Training Loss: 4.507374505738954, Validation Loss: 4.998869895935059\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 97%|█████████▋| 29/30 [38:19<02:01, 121.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/30, Training Loss: 4.471399784088135, Validation Loss: 4.98656302690506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 30/30 [38:49<00:00, 77.65s/it] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/30, Training Loss: 4.435467127207163, Validation Loss: 4.970628440380096\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 30  # Set the number of epochs\n",
    "\n",
    "for epoch in tqdm(range(num_epochs)):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "\n",
    "    for inputs, targets in train_loader:\n",
    "        hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "        optimizer.zero_grad()  # Zero the gradients\n",
    "\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.transpose(1, 2), targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        optimizer.step()  # Update weights\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    total_val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in val_loader:\n",
    "            val_hidden = model.init_hidden(inputs.size(0))\n",
    "            outputs, val_hidden = model(inputs, val_hidden)\n",
    "            val_loss = criterion(outputs.transpose(1, 2), targets)\n",
    "            total_val_loss += val_loss.item()\n",
    "\n",
    "    # Print average training and validation loss\n",
    "    print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {total_loss/len(train_loader)}, Validation Loss: {total_val_loss/len(val_loader)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "30 epochs was enough without the punctuation but for this more training will be helpful. Dont have the time to train more rn "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average Test Loss: 5.018877923488617\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # Set the model to evaluation mode\n",
    "test_loss = 0\n",
    "\n",
    "with torch.no_grad():  # Disable gradient computations\n",
    "    for inputs, targets in test_loader:\n",
    "        hidden = model.init_hidden(inputs.size(0))  # Initialize hidden state\n",
    "        outputs, hidden = model(inputs, hidden)\n",
    "        loss = criterion(outputs.transpose(1, 2), targets)  # Calculate loss\n",
    "        test_loss += loss.item()\n",
    "\n",
    "average_test_loss = test_loss / len(test_loader)\n",
    "print(f'Average Test Loss: {average_test_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "\n",
    "# def generate_sentence(model, seed_text, max_length, vocab, idx_to_word):\n",
    "#     model.eval()  # Evaluation mode\n",
    "#     hidden = model.init_hidden(1)  # Initialize hidden state for single sequence\n",
    "\n",
    "#     current_sequence = [vocab.get(word, vocab['<UNK>']) for word in seed_text.split()]  # Handle unknown words\n",
    "#     generated_sentence = seed_text\n",
    "\n",
    "#     for _ in range(max_length - len(current_sequence)):\n",
    "#         # Convert current_sequence to PyTorch tensor\n",
    "#         sequence_tensor = torch.tensor([current_sequence], dtype=torch.long)\n",
    "        \n",
    "#         # Get model prediction\n",
    "#         with torch.no_grad():\n",
    "#             output, hidden = model(sequence_tensor, hidden)\n",
    "\n",
    "#         # Get the last word of the output\n",
    "#         last_word_logits = output[0, -1]\n",
    "#         predicted_index = torch.argmax(last_word_logits).item()\n",
    "#         predicted_word = idx_to_word[predicted_index]\n",
    "\n",
    "#         if predicted_word == '<\\s>':  # Assuming you have an end token\n",
    "#             break\n",
    "\n",
    "#         generated_sentence += ' ' + predicted_word\n",
    "#         current_sequence.append(predicted_index)\n",
    "#         current_sequence = current_sequence[1:]  # Move the window\n",
    "\n",
    "#     return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code doesnt have randomness, predicts the same word everytime "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def generate_sentence(model, seed_text, max_length, vocab, idx_to_word, temperature=1.0):\n",
    "    model.eval()  # Evaluation mode\n",
    "    hidden = model.init_hidden(1)  # Initialize hidden state for single sequence\n",
    "\n",
    "    current_sequence = [vocab.get(word, vocab['<UNK>']) for word in seed_text.split()]\n",
    "    generated_sentence = seed_text\n",
    "\n",
    "    for _ in range(max_length - len(current_sequence)):\n",
    "        sequence_tensor = torch.tensor([current_sequence], dtype=torch.long)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            output, hidden = model(sequence_tensor, hidden)\n",
    "\n",
    "        # Apply softmax and sample from the distribution\n",
    "        probabilities = F.softmax(output[0, -1] / temperature, dim=0).cpu().numpy()\n",
    "        predicted_index = np.random.choice(len(probabilities), p=probabilities)\n",
    "        predicted_word = idx_to_word[predicted_index]\n",
    "\n",
    "        if predicted_word == '</s>':\n",
    "            break\n",
    "\n",
    "        generated_sentence += ' ' + predicted_word\n",
    "        current_sequence.append(predicted_index)\n",
    "        current_sequence = current_sequence[1:]\n",
    "\n",
    "    return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has randomness as seen from the Sentence outputs below "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for word, idx in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <s> no there might her most there of their life .\n",
      "Sentence 2: <s> the croft almost also she considered years in again : in each of the smile as seriously wants as intercourse beyond ; he is now , which sensations my more of those , brilliancy her wife how .\n",
      "Sentence 3: <s> the hill and shame !\n",
      "Sentence 4: <s> the credit park in fields at an soon\n",
      "Sentence 5: <s> the intervals were finding by evening birth each life as not be claim\n",
      "Sentence 6: <s> the to instance myself to give\n",
      "Sentence 7: <s> the invitation object are an point .\n",
      "Sentence 8: <s> the more view and her had intended a order to see every .\n",
      "Sentence 9: <s> the historian which had difficult advantage to model twice , fix the in visits , nothing were compliance again ; but i must be longer , has soon my tone ; she could regard to plain to home .\n",
      "Sentence 10: <s> the few staying of home acquaintance , not never have been afraid since an interchange , and gave and laura to gratify him\n",
      "Sentence 11: <s> the almost of possession , and far if if elliot as you walked by this dignity of twenty ; he would come occasionally unimportant\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\"<s>\", \"<s> the croft\", \"<s> the hill\", \"<s> the\", \"<s> the\" , \"<s> the\", \n",
    "              \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\"]\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "for seed in seed_texts:\n",
    "    sentence = generate_sentence(model, seed, 45, vocabulary, idx_to_word)\n",
    "    generated_sentences.append(sentence)\n",
    "\n",
    "for i, sentence in enumerate(generated_sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of output when trained without the punctuation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <s> i have not been introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 2: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 3: <s> the hill was the same of her father </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 4: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 5: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 6: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 7: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 8: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 9: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 10: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n",
      "Sentence 11: <s> the crofts was not to be introduced </s> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\"<s>\", \"<s> the\", \"<s> the hill\", \"<s> the\", \"<s> the\" , \"<s> the\", \n",
    "              \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\"]\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "for seed in seed_texts:\n",
    "    sentence = generate_sentence_beam_search(model, seed, 45, vocabulary, idx_to_word)\n",
    "    generated_sentences.append(sentence)\n",
    "\n",
    "for i, sentence in enumerate(generated_sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def generate_sentence_beam_search(model, seed_text, max_length, vocab, idx_to_word, beam_width=3):\n",
    "#     model.eval()\n",
    "#     hidden = model.init_hidden(1)\n",
    "\n",
    "#     # Initialize the beam as a list of tuples (sequence, score)\n",
    "#     start_idx = [vocab.get(word, vocab['<UNK>']) for word in seed_text.split()]\n",
    "#     beam = [(start_idx, 0)]\n",
    "\n",
    "#     for _ in range(max_length - len(start_idx)):\n",
    "#         candidates = []\n",
    "\n",
    "#         for seq, score in beam:\n",
    "#             sequence_tensor = torch.tensor([seq], dtype=torch.long)\n",
    "#             with torch.no_grad():\n",
    "#                 output, hidden = model(sequence_tensor, hidden)\n",
    "\n",
    "#             probabilities = F.softmax(output[0, -1], dim=0).cpu().numpy()\n",
    "\n",
    "#             # Consider all possible next words and their scores\n",
    "#             for idx, prob in enumerate(probabilities):\n",
    "#                 next_seq = seq + [idx]\n",
    "#                 next_score = score + np.log(prob)  # Use log probabilities for numerical stability\n",
    "#                 candidates.append((next_seq, next_score))\n",
    "\n",
    "#         # Keep only the top beam_width sequences\n",
    "#         ordered = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "#         beam = ordered[:beam_width]\n",
    "\n",
    "#     # Select the sequence with the highest score\n",
    "#     best_sequence, _ = max(beam, key=lambda x: x[1])\n",
    "\n",
    "#     # Convert sequence of indices to words\n",
    "#     generated_sentence = ' '.join(idx_to_word[idx] for idx in best_sequence if idx in idx_to_word)\n",
    "\n",
    "#     return generated_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this had some issues, improved it below "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Temperature Parameter: This parameter is used to control the randomness in the prediction process. A higher temperature results in more random outputs, while a lower temperature leads to more predictable outputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_sentence_beam_search(model, seed_text, max_length, vocab, idx_to_word, temp = 1.0, beam_width=10):\n",
    "    model.eval()\n",
    "    hidden = model.init_hidden(1)\n",
    "\n",
    "    # Initialize the beam as a list of tuples (index_seq, score)\n",
    "    index_seq = [vocab.get(word, vocab['<UNK>']) for word in seed_text.split()]\n",
    "    beam = [(index_seq, 0)]\n",
    "\n",
    "    for _ in range(max_length - len(index_seq)): # note: could stop earlier, but this isn't slow\n",
    "        candidates = []\n",
    "\n",
    "        for seq, score in beam:\n",
    "            sequence_tensor = torch.tensor([seq], dtype=torch.long)\n",
    "            with torch.no_grad():\n",
    "                output, hidden = model(sequence_tensor, hidden)\n",
    "\n",
    "            probabilities = F.softmax(output[0, -1] / temp, dim=0).numpy()\n",
    "\n",
    "            # Consider all possible next words and their scores\n",
    "            for idx, prob in enumerate(probabilities):\n",
    "                END_OF_SENT_IDX = 2\n",
    "                if seq[-1] == END_OF_SENT_IDX and idx != END_OF_SENT_IDX:\n",
    "                    # note: this is impossible\n",
    "                    continue\n",
    "                else:\n",
    "                    # perplexity calculation should incentivize stopping earlier than later, without over-incentivizing it, which this dampening\n",
    "                    # calculation is doing -- technically the perplexity_len could be any positive integer and would have a noticeable effect\n",
    "                    if idx == END_OF_SENT_IDX and seq[-1] == END_OF_SENT_IDX:   \n",
    "                        perplexity_len = len(seq)   # NOTE: len(seq) = len(next_seq) - 1 as would be sought, but just using here to dampen probability val\n",
    "                        next_score = score + (np.log(prob) / perplexity_len)   # note: technically exponents become factors of multiplication upon log\n",
    "                    else:\n",
    "                        next_score = score + np.log(prob)\n",
    "                    next_seq = seq + [idx]\n",
    "                    candidates.append((next_seq, next_score))\n",
    "\n",
    "        # Keep only the top beam_width sequences\n",
    "        ordered = sorted(candidates, key=lambda x: x[1], reverse=True)\n",
    "        beam = ordered[:beam_width]\n",
    "\n",
    "\n",
    "    # Convert sequences of indices to strings of words\n",
    "    beam_of_sentences = []\n",
    "    for sentence_list in beam:\n",
    "        generated_sentence = ' '.join(idx_to_word[idx] for idx in sentence_list[0] if idx in idx_to_word)\n",
    "        beam_of_sentences.append(generated_sentence)\n",
    "\n",
    "    return beam_of_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_to_word = {idx: word for word, idx in vocabulary.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: <s> she wanted , however could be strong .\n",
      "Sentence 2: <s> the croft she did been uttered of her man , maintaining by the attention - the other she had he of by to humoured her .\n",
      "Sentence 3: <s> the hill at her which not recommend if captain wentworth , and seems of the likely enquiries for her and expressly to comprise for warm of excellent , which an any gentleman .\n",
      "Sentence 4: <s> the very happy had as they , had no tender , and quickly :\n",
      "Sentence 5: <s> the young emotions , may not seem .\n",
      "Sentence 6: <s> the first one spirits and the frightful an apparent observing acquaintance .\n",
      "Sentence 7: <s> the house musgroves in the good , as if unjust ,\n",
      "Sentence 8: <s> the country william ' s\n",
      "Sentence 9: <s> the little thing reached the evening obtrusiveness and a son and declining as a laconia , with offers saying him by itself component what - modes and this character of air ( and mr suddenly to have able met to omit desirable brother ,\n",
      "Sentence 10: <s> the feelings , whether anne , hardly what in\n",
      "Sentence 11: <s> the roused only an these - uppercross , when and the evil might been not , and if i believe he could recollect quite to desirable again .\n"
     ]
    }
   ],
   "source": [
    "seed_texts = [\"<s>\", \"<s> the croft\", \"<s> the hill\", \"<s> the\", \"<s> the\" , \"<s> the\", \n",
    "              \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\", \"<s> the\"]\n",
    "\n",
    "generated_sentences = []\n",
    "\n",
    "for seed in seed_texts:\n",
    "    sentence = generate_sentence(model, seed, 45, vocabulary, idx_to_word)\n",
    "    generated_sentences.append(sentence)\n",
    "\n",
    "for i, sentence in enumerate(generated_sentences, 1):\n",
    "    print(f\"Sentence {i}: {sentence}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below sentences are generateed without beam search. We can see that after beam search the punctuations are better "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence 1: <s> no there might her most there of their life .\n",
    "# Sentence 2: <s> the croft almost also she considered years in again : in each of the smile as seriously wants as intercourse beyond ; he is now , which sensations my more of those , brilliancy her wife how .\n",
    "# Sentence 3: <s> the hill and shame !\n",
    "# Sentence 4: <s> the credit park in fields at an soon\n",
    "# Sentence 5: <s> the intervals were finding by evening birth each life as not be claim\n",
    "# Sentence 6: <s> the to instance myself to give\n",
    "# Sentence 7: <s> the invitation object are an point .\n",
    "# Sentence 8: <s> the more view and her had intended a order to see every .\n",
    "# Sentence 9: <s> the historian which had difficult advantage to model twice , fix the in visits , nothing were compliance again ; but i must be longer , has soon my tone ; she could regard to plain to home .\n",
    "# Sentence 10: <s> the few staying of home acquaintance , not never have been afraid since an interchange , and gave and laura to gratify him\n",
    "# Sentence 11: <s> the almost of possession , and far if if elliot as you walked by this dignity of twenty ; he would come occasionally unimportant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Asked ChatGPT how good the sentences generated by my word level language model were. This was the response:\n",
    "\n",
    "Vocabulary and Structure: The sentences reflect a good range of vocabulary and mimic the structure of natural language to some extent. They generally start with a clear opening and contain a mix of simple and complex structures.\n",
    "\n",
    "Contextual and Logical Flow: Many sentences lack logical consistency and contextual clarity. For example, Sentence 9 is quite lengthy and seems to lose coherence partway through.\n",
    "\n",
    "Relevance and Completeness: Some sentences (like Sentences 4, 5, 6, and 7) are incomplete or abruptly end, indicating a potential issue with how the model handles sentence termination.\n",
    "\n",
    "Grammatical Accuracy: While some sentences are grammatically correct, others contain errors or awkward constructions.\n",
    "\n",
    "In summary, while the model demonstrates an ability to generate sentences with a variety of structures and vocabulary, the overall coherence, logical flow, and grammatical accuracy are mixed. Fine-tuning the model, adjusting hyperparameters, or training on a more diverse or larger dataset might improve performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Three:  Part-of-Speech Tagging (40 pts)\n",
    "\n",
    "In this problem, we will experiment with three different approaches to the POS tagging problem, using\n",
    "the Brown Corpus as our data set. \n",
    "\n",
    "Before starting this problem, please review Lecture 13 and download the file <a href=\"https://www.cs.bu.edu/fac/snyder/cs505/Viterbi_Algorithm.ipynb\">Viterbi_Algorithm.ipynb</a> from the \n",
    "class web site. \n",
    "\n",
    "There are four parts to this problem:\n",
    "\n",
    "- Part A: You will establish a baseline accuracy for the task. \n",
    "- Part B: Using the implementation of the Viterbi algorithm for Hidden Markov Models you downloaded, you will determine how much better than the baseline you can do with this very standard method.\n",
    "- Part C: You will repeat the exercise of Part B, but using an LSTM implementation, exploring several options for the implementation of the LSTM layer.\n",
    "- Part D: You will evaluate your results, comparing the various methods in the context of the baseline method from Part A.\n",
    "- Optional: You may wish to try the same task with a transformer such as Bert. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the Brown Corpus has a list of all sentences tagged with parts of speech. The tags are\n",
    "a bit odd, and not generally used any more, so we will use a much simpler set of tags the `universal_tagset`. \n",
    "\n",
    "If you run the following cells, you will see that there are 57,340 sentences, tagged with 12 different tags. \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 57340 sentences tagged with universal POS tags in the Brown Corpus.\n",
      "\n",
      "Here is the first sentence with universal tags: [('The', 'DET'), ('Fulton', 'NOUN'), ('County', 'NOUN'), ('Grand', 'ADJ'), ('Jury', 'NOUN'), ('said', 'VERB'), ('Friday', 'NOUN'), ('an', 'DET'), ('investigation', 'NOUN'), ('of', 'ADP'), (\"Atlanta's\", 'NOUN'), ('recent', 'ADJ'), ('primary', 'NOUN'), ('election', 'NOUN'), ('produced', 'VERB'), ('``', '.'), ('no', 'DET'), ('evidence', 'NOUN'), (\"''\", '.'), ('that', 'ADP'), ('any', 'DET'), ('irregularities', 'NOUN'), ('took', 'VERB'), ('place', 'NOUN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "# The first time you will need to download the corpus:\n",
    "\n",
    "from nltk.corpus import brown\n",
    " \n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "\n",
    "tagged_sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "print(f'There are {len(tagged_sentences)} sentences tagged with universal POS tags in the Brown Corpus.')\n",
    "print(\"\\nHere is the first sentence with universal tags:\",tagged_sentences[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 12 universal tags in the Brown Corpus.\n",
      "['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Uncomment to see the complete list of tags. \n",
    "\n",
    "all_tagged_words = np.concatenate(tagged_sentences)\n",
    "all_tags = sorted(set([pos for (w,pos) in all_tagged_words]))\n",
    "print(f'There are {len(all_tags)} universal tags in the Brown Corpus.')\n",
    "print(all_tags)\n",
    "print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['The' 'DET']\n",
      " ['Fulton' 'NOUN']\n",
      " ['County' 'NOUN']\n",
      " ...\n",
      " ['was' 'VERB']\n",
      " ['stupefying' 'VERB']\n",
      " ['.' '.']]\n"
     ]
    }
   ],
   "source": [
    "print(all_tagged_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VnB5z-ZoZEa5"
   },
   "source": [
    "### Part A\n",
    "\n",
    "In this part, you will establish a baseline for the task, using the naive method suggested on slide 35 of Lecture 13:\n",
    "\n",
    "- Tag every word with its most frequent POS tag (for example, if 'recent' is most frequently tagged as 'ADJ', then assume that every time 'recent' appears in a sentence, it should be tagged with 'ADJ'); \n",
    "- If a word has two or more most frequent tags, choose the one that appears first in the list of sorted tags above. \n",
    "\n",
    "Note that there will not be any \"unknown words.\" \n",
    " \n",
    "Use this method to determine your baseline accuracy (it may not be 92% as reported on slide 35!):\n",
    "\n",
    "- Build a dictionary mapping every word to its most frequent tag;\n",
    "- Go through the entire tagged corpus, and report the accuracy (percentage of correct tags) of this baseline method. \n",
    "\n",
    "Do not tokenize or lower-case the words. Use the words and tags exactly as they are in the tagged sentences. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_tags = brown.tagged_words(tagset='universal')\n",
    "\n",
    "# Counting the frequency of each tag for each word\n",
    "tag_freq = defaultdict(Counter)\n",
    "for word, tag in words_tags:\n",
    "    tag_freq[word][tag] += 1  \n",
    "\n",
    "# The list of sorted tags provided by the user\n",
    "user_defined_order = ['.', 'ADJ', 'ADP', 'ADV', 'CONJ', 'DET', 'NOUN', 'NUM', 'PRON', 'PRT', 'VERB', 'X']\n",
    "\n",
    "# Determining the most frequent tag for each word\n",
    "# If a tie exists, choose the tag that appears first in the user-defined order\n",
    "most_freq_tags = {}\n",
    "for word, counter in tag_freq.items():\n",
    "    most_common_tags = counter.most_common()\n",
    "    max_freq = most_common_tags[0][1]\n",
    "    tied_tags = [tag for tag, freq in most_common_tags if freq == max_freq]\n",
    "\n",
    "    # Sort tied tags based on the user-defined order and select the first one\n",
    "    sorted_tied_tags = sorted(tied_tags, key=lambda tag: user_defined_order.index(tag))\n",
    "    most_freq_tags[word] = sorted_tied_tags[0]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'DET'),\n",
       " ('quick', 'ADJ'),\n",
       " ('brown', 'ADJ'),\n",
       " ('fox', 'NOUN'),\n",
       " ('jumps', 'NOUN'),\n",
       " ('over', 'ADP'),\n",
       " ('the', 'DET'),\n",
       " ('lazy', 'ADJ'),\n",
       " ('dog', 'NOUN')]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example of tagging a new sentence\n",
    "sample_sentence = \"The quick brown fox jumps over the lazy dog\".split()\n",
    "tagged_sentence = [(word, most_freq_tags.get(word, 'NOUN')) for word in sample_sentence]\n",
    "\n",
    "tagged_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " If a word is not found in the dictionary, it defaults to 'NOUN'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94.69467581588574"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Tagging the entire corpus with the most frequent tags\n",
    "re_tagged_corpus = [(word, most_freq_tags.get(word.lower(), 'NOUN')) for word, _ in words_tags]\n",
    "\n",
    "# Calculating accuracy\n",
    "correct_tags = sum(original_tag == re_tagged_tag for (_, original_tag), (_, re_tagged_tag) in zip(words_tags, re_tagged_corpus))\n",
    "accuracy = correct_tags / len(words_tags) * 100\n",
    "\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "94.69 is pretty good accuracy. This shows that most words just follow their most common POS tagged. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part B:  \n",
    "\n",
    "Now, review the `Viterbi.ipynb` notebook and read through Section 8.4 in Jurafsky & Martin to understand the basic approach that is used in the \"Janet will back the bill\" example. In detail:\n",
    "\n",
    "- Cut and paste the code from the Viterby notebook below and run your experiments in this notebook. \n",
    "- You need to calculate from the Brown Corpus tagged sentences the probabilities for the various matrices used as input to the method:\n",
    "   - `start_p`: This is the probability that a sentence starts with a given POS (in Figure 8.12 in J & M, this is given as the first line, in the row for `<s>`; simply collect the statistics for the first word in each sentence; it will be of size 1 x 12. \n",
    "   - `trans_p`: This is the matrix of probabilities that one POS follows another in a sentence; build a 12 x 12 matrix of frequencies for whether the column POS follows the row POS in a sentence and then normalize each row so that it is a probability distribution (each row should add to 1.0)\n",
    "   - `emit_p`: This is a matrix of size 12 x N, where N is the number of unique words in the corpus, which for each POS (the row) gives the probability that this POS in the output sequence corresponds to a specific word (the column) in the input sequence; again, you should collect frequency statistics about the relationship between POS and words, and normalize so that every row sums to 1.0. \n",
    "   \n",
    "Then run the algorithm on all the sentences in the tagged corpus, and determine the accuracy of the Viterbi algorithm. Again, the accuracy is calculated on each word, not on sentences as a whole. \n",
    "\n",
    "Report your results as a raw accuracy score, and in the two ways that were suggested on slide 12 of Lecture 11: percentage above the baseline established in Part A, and Cohen's Kappa. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Viterbi code should be pasted here\n",
    "\n",
    "# def viterbi(obs_sequence, obs, states, start_p, trans_p, emit_p, logspace=True):\n",
    "    \n",
    "#     V = [{}]\n",
    "#     for st in states:\n",
    "#         # Check if the word is in the emission probabilities and handle unknown words\n",
    "#         emit_prob = emit_p[st].get(obs_sequence[0].lower(), float('-inf')) if logspace else emit_p[st].get(obs_sequence[0].lower(), 0)\n",
    "#         if logspace:\n",
    "#             V[0][st] = {\"prob\": start_p[st] + emit_prob, \"prev\": None}\n",
    "#         else:\n",
    "#             V[0][st] = {\"prob\": start_p[st] * emit_prob, \"prev\": None}\n",
    "        \n",
    "#     # Run Viterbi when t > 0\n",
    "#     for t in range(1, len(obs_sequence)):\n",
    "#         V.append({})\n",
    "#         for st in states:\n",
    "#             max_tr_prob, prev_st_selected = None, None\n",
    "#             for prev_st in states:\n",
    "#                 tr_prob = V[t - 1][prev_st][\"prob\"] + trans_p[prev_st].get(st, float('-inf')) if logspace else V[t - 1][prev_st][\"prob\"] * trans_p[prev_st].get(st, 0)\n",
    "#                 if max_tr_prob is None or tr_prob > max_tr_prob:\n",
    "#                     max_tr_prob, prev_st_selected = tr_prob, prev_st\n",
    "\n",
    "#             emit_prob = emit_p[st].get(obs_sequence[t].lower(), float('-inf')) if logspace else emit_p[st].get(obs_sequence[t].lower(), 0)\n",
    "#             max_prob = max_tr_prob + emit_prob if logspace else max_tr_prob * emit_prob\n",
    "#             V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "#     # Backtracking to find the best path\n",
    "#     opt, max_prob, best_st = [], float('-inf'), None\n",
    "#     for st, data in V[-1].items():\n",
    "#         if data[\"prob\"] > max_prob:\n",
    "#             max_prob, best_st = data[\"prob\"], st\n",
    "#     opt.append(best_st)\n",
    "#     previous = best_st\n",
    "\n",
    "#     for t in range(len(V) - 2, -1, -1):\n",
    "#         opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "#         previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "#     return opt, max_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Counting the frequency of each tag as the first tag in a sentence\n",
    "first_tag_freq = defaultdict(int)\n",
    "for sentence in tagged_sentences:\n",
    "    if sentence:  # Check if the sentence is not empty\n",
    "        first_tag = sentence[0][1]  # Get the tag of the first word in the sentence\n",
    "        first_tag_freq[first_tag] += 1\n",
    "\n",
    "# Calculating the total number of sentences\n",
    "total_sentences = len(tagged_sentences)\n",
    "\n",
    "# Calculating the probability of each tag being the first tag\n",
    "start_p = {tag: count / total_sentences for tag, count in first_tag_freq.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DET': 0.21342867108475758,\n",
       " '.': 0.08892570631321939,\n",
       " 'PRON': 0.15969654691314963,\n",
       " 'NOUN': 0.1411405650505755,\n",
       " 'ADV': 0.0913498430415068,\n",
       " 'ADP': 0.1228461806766655,\n",
       " 'VERB': 0.04513428671084758,\n",
       " 'NUM': 0.016811998604813395,\n",
       " 'CONJ': 0.049128008371119636,\n",
       " 'ADJ': 0.034339030345308684,\n",
       " 'PRT': 0.036675967910708054,\n",
       " 'X': 0.0005231949773282176}"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These probabilities sum up to 1. Now we will convert these to log probabilities "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_p_log = {}\n",
    "for tag, prob in start_p.items():\n",
    "    if prob > 0:\n",
    "        start_p_log[tag] = np.log(prob)\n",
    "    else:\n",
    "        start_p_log[tag] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'DET': -1.5444525954148751,\n",
       " '.': -2.4199540183822053,\n",
       " 'PRON': -1.8344798463296288,\n",
       " 'NOUN': -1.957998969941659,\n",
       " 'ADV': -2.3930587143697037,\n",
       " 'ADP': -2.096822269821264,\n",
       " 'VERB': -3.09811308381657,\n",
       " 'NUM': -4.0856624448268155,\n",
       " 'CONJ': -3.013325971560988,\n",
       " 'ADJ': -3.3714726607982355,\n",
       " 'PRT': -3.305633563734661,\n",
       " 'X': -7.555556357775205}"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_p_log "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These obviously dont sum up to 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This approach maintains the relative differences between probabilities while making them more manageable for computational purposes, especially when dealing with products of multiple probabilities in sequence models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Populate the transition matrix with frequencies\n",
    "for sentence in tagged_sentences:\n",
    "    for i in range(len(sentence) - 1):\n",
    "        current_tag = sentence[i][1]\n",
    "        next_tag = sentence[i + 1][1]\n",
    "        trans_matrix[current_tag][next_tag] += 1\n",
    "\n",
    "# Normalize each row to convert frequencies to probabilities\n",
    "trans_p = defaultdict(dict)\n",
    "for current_tag, next_tag_counts in trans_matrix.items():\n",
    "    total_counts = sum(next_tag_counts.values())\n",
    "    for next_tag, count in next_tag_counts.items():\n",
    "        trans_p[current_tag][next_tag] = count / total_counts if total_counts > 0 else 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This trans_p matrix represents the transition probabilities between tags and is a crucial component for the Viterbi algorithm in POS tagging, as it quantifies how likely a tag is to follow another in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_p \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans_p_log = defaultdict(dict)\n",
    "for current_tag, next_tag_probs in trans_p.items():\n",
    "    for next_tag, prob in next_tag_probs.items():\n",
    "        if prob > 0:\n",
    "            trans_p_log[current_tag][next_tag] = np.log(prob)\n",
    "        else:\n",
    "            trans_p_log[current_tag][next_tag] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans_p_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_words = brown.tagged_words(tagset='universal')\n",
    "\n",
    "# Initialize the emission matrix\n",
    "emit_matrix = defaultdict(lambda: defaultdict(int))\n",
    "\n",
    "# Populate the emission matrix with frequencies\n",
    "for word, tag in tagged_words:\n",
    "    emit_matrix[tag][word.lower()] += 1  # Using lower case for generalization\n",
    "\n",
    "# Normalize each row to convert frequencies to probabilities\n",
    "emit_p = defaultdict(dict)\n",
    "for tag, word_counts in emit_matrix.items():\n",
    "    total_counts = sum(word_counts.values())\n",
    "    for word, count in word_counts.items():\n",
    "        emit_p[tag][word] = count / total_counts if total_counts > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit_p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This emit_p matrix is a key component of the Hidden Markov Model (HMM) for POS tagging, as it quantifies the likelihood of each word being generated by each POS tag.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "emit_p_log = defaultdict(dict)\n",
    "for tag, word_probs in emit_p.items():\n",
    "    for word, prob in word_probs.items():\n",
    "        if prob > 0:\n",
    "            emit_p_log[tag][word] = np.log(prob)\n",
    "        else:\n",
    "            emit_p_log[tag][word] = float('-inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# emit_p_log"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def viterbi(obs_sequence, obs, states, start_p, trans_p, emit_p, logspace=True):\n",
    "    \n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        # Check if the word is in the emission probabilities and handle unknown words\n",
    "        emit_prob = emit_p[st].get(obs_sequence[0].lower(), float('-inf')) if logspace else emit_p[st].get(obs_sequence[0].lower(), 0)\n",
    "        if logspace:\n",
    "            V[0][st] = {\"prob\": start_p[st] + emit_prob, \"prev\": None}\n",
    "        else:\n",
    "            V[0][st] = {\"prob\": start_p[st] * emit_prob, \"prev\": None}\n",
    "        \n",
    "    # Run Viterbi when t > 0\n",
    "    for t in range(1, len(obs_sequence)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob, prev_st_selected = None, None\n",
    "            for prev_st in states:\n",
    "                tr_prob = V[t - 1][prev_st][\"prob\"] + trans_p[prev_st].get(st, float('-inf')) if logspace else V[t - 1][prev_st][\"prob\"] * trans_p[prev_st].get(st, 0)\n",
    "                if max_tr_prob is None or tr_prob > max_tr_prob:\n",
    "                    max_tr_prob, prev_st_selected = tr_prob, prev_st\n",
    "\n",
    "            emit_prob = emit_p[st].get(obs_sequence[t].lower(), float('-inf')) if logspace else emit_p[st].get(obs_sequence[t].lower(), 0)\n",
    "            max_prob = max_tr_prob + emit_prob if logspace else max_tr_prob * emit_prob\n",
    "            V[t][st] = {\"prob\": max_prob, \"prev\": prev_st_selected}\n",
    "\n",
    "    # Backtracking to find the best path\n",
    "    opt, max_prob, best_st = [], float('-inf'), None\n",
    "    for st, data in V[-1].items():\n",
    "        if data[\"prob\"] > max_prob:\n",
    "            max_prob, best_st = data[\"prob\"], st\n",
    "    opt.append(best_st)\n",
    "    previous = best_st\n",
    "\n",
    "    for t in range(len(V) - 2, -1, -1):\n",
    "        opt.insert(0, V[t + 1][previous][\"prev\"])\n",
    "        previous = V[t + 1][previous][\"prev\"]\n",
    "\n",
    "    return opt, max_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Algorithm steps \n",
    "\n",
    "nitialization: The first step initializes the Viterbi path probability for each state (POS tag) at the first observation (word in the sentence) based on start probabilities (start_p) and emission probabilities (emit_p). The algorithm correctly handles unknown words by assigning the lowest possible probability (or zero in non-log space).\n",
    "\n",
    "Iteration Over Observations: For each subsequent word in the observed sequence, it calculates the maximum probability path to each state, considering the transition probabilities (trans_p) and emission probabilities.\n",
    "\n",
    "Backtracking: After the forward pass through the observation sequence, the algorithm backtracks to find the most probable path of states (POS tags) for the entire sequence.\n",
    "\n",
    "Logarithmic Mode: The algorithm can operate in logarithmic space (logspace=True) to handle the small probabilities effectively and avoid underflow, a common issue in such computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 97.39%\n"
     ]
    }
   ],
   "source": [
    "correct_tags = 0\n",
    "total_tags = 0\n",
    "states = set(tag for word, tag in tagged_words)\n",
    "states = list(states)\n",
    "\n",
    "for sentence in tagged_sentences:\n",
    "    # Extract words and actual tags\n",
    "    words = [word for word, tag in sentence]\n",
    "    actual_tags = [tag for word, tag in sentence]\n",
    "\n",
    "    # Run Viterbi algorithm with log probabilities\n",
    "    predicted_tags, _ = viterbi(words, words, states, start_p_log, trans_p_log, emit_p_log, logspace=True)\n",
    "\n",
    "    # Compare predicted tags with actual tags\n",
    "    correct_tags += sum(p_tag == a_tag for p_tag, a_tag in zip(predicted_tags, actual_tags))\n",
    "    total_tags += len(sentence)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = correct_tags / total_tags if total_tags > 0 else 0\n",
    "\n",
    "print(f\"Accuracy: {accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement over baseline = 2.7%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.700000000000003"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "97.39 - 94.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cohens_kappa(accuracy, baseline_accuracy):\n",
    "    num = accuracy-baseline_accuracy\n",
    "    den = 1.0 - baseline_accuracy \n",
    "    return num/den"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5084745762711865\n"
     ]
    }
   ],
   "source": [
    "print(cohens_kappa(.9739, .9469))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "closer to 1 is desired but .508 suggests moderate agreement \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part C:  \n",
    "\n",
    "Next, you will need to develop an LSTM model to solve this problem. You may find it useful to\n",
    "refer to the following, which presents an approach in Keras.\n",
    "\n",
    "https://www.kaggle.com/code/tanyadayanand/pos-tagging-using-rnn/notebook\n",
    "\n",
    "\n",
    "You must do the following for this part:\n",
    "\n",
    "- Develop your code in Pytorch (of course!);\n",
    "- Use pretrained GloVe embeddings of dimension 200 and update them with the brown sentences; if you run into problems with RAM, you may use a smaller embedding dimension; \n",
    "- Truncate all sentences to a maximum of length 100 tokens, and pad shorter sentences (as in the reference above);\n",
    "- Use an LSTM model and try several different choices for the parameters to the layer:\n",
    "  - `hidden_size`:  Try several different widths for the layer\n",
    "  - `bidirectional`: Try unidirectional (False) and bidirectional (True)\n",
    "  - `num_layers`: Try 1 layer and 2 layers\n",
    "  - `dropout`: In the case of 2 layers, try several different dropouts, including 0.\n",
    "- Use early stopping with `patience = 50`;  \n",
    "You do not have to try every possible combination of these parameter choices; a good strategy is to\n",
    "try them separately, and then try a couple of combinations of the best choices of each. \n",
    "\n",
    "It is your choice about the other hyperparameters.  \n",
    "\n",
    "Provide a brief discussion of what you discovered, your best loss and accuracy measures for\n",
    "validation, and three versions of your testing accuracy, as in Part B.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 0.4415715837229202, Validation Loss: 0.31242254885231574\n",
      "Epoch 2, Train Loss: 0.2373056224017114, Validation Loss: 0.1845006322299977\n",
      "Epoch 3, Train Loss: 0.1614218174164048, Validation Loss: 0.14181793449959273\n",
      "Epoch 4, Train Loss: 0.13103973445236528, Validation Loss: 0.1200516473904304\n",
      "Epoch 5, Train Loss: 0.1135922578904768, Validation Loss: 0.10621499163763863\n",
      "Epoch 6, Train Loss: 0.10208852928206519, Validation Loss: 0.09679583417258196\n",
      "Epoch 7, Train Loss: 0.09403508140948304, Validation Loss: 0.08997246706111921\n",
      "Epoch 8, Train Loss: 0.0880648998905756, Validation Loss: 0.08479324415429959\n",
      "Epoch 9, Train Loss: 0.08339034851052601, Validation Loss: 0.08063952806519299\n",
      "Epoch 10, Train Loss: 0.07954508977850416, Validation Loss: 0.07720213663806483\n",
      "Epoch 11, Train Loss: 0.07626716062140756, Validation Loss: 0.07415783426072124\n",
      "Epoch 12, Train Loss: 0.07336943526123021, Validation Loss: 0.07148689746804769\n",
      "Epoch 13, Train Loss: 0.0707603965600938, Validation Loss: 0.06905944549352035\n",
      "Epoch 14, Train Loss: 0.06837005560215702, Validation Loss: 0.06681932721819196\n",
      "Epoch 15, Train Loss: 0.06614340785582272, Validation Loss: 0.0647114505050282\n",
      "Epoch 16, Train Loss: 0.06406276907898492, Validation Loss: 0.06272005854427606\n",
      "Epoch 17, Train Loss: 0.06210866872523282, Validation Loss: 0.06085360299419443\n",
      "Epoch 18, Train Loss: 0.06026349133554498, Validation Loss: 0.05913479332828356\n",
      "Epoch 19, Train Loss: 0.05852998843142128, Validation Loss: 0.057512512281277454\n",
      "Epoch 20, Train Loss: 0.056891589983323394, Validation Loss: 0.05595372320000303\n",
      "Epoch 21, Train Loss: 0.055353792938026225, Validation Loss: 0.05447604054202186\n",
      "Epoch 22, Train Loss: 0.05390307808062671, Validation Loss: 0.053143224065623634\n",
      "Epoch 23, Train Loss: 0.05253080493967847, Validation Loss: 0.05182574628381779\n",
      "Epoch 24, Train Loss: 0.051249345413749384, Validation Loss: 0.05064835806315576\n",
      "Epoch 25, Train Loss: 0.05004068419057957, Validation Loss: 0.04950602923743816\n",
      "Epoch 26, Train Loss: 0.04890580599236041, Validation Loss: 0.048437679815998474\n",
      "Epoch 27, Train Loss: 0.04782826010115061, Validation Loss: 0.047465667874918994\n",
      "Epoch 28, Train Loss: 0.04681751259814529, Validation Loss: 0.04652351063095319\n",
      "Epoch 29, Train Loss: 0.04586749420109113, Validation Loss: 0.04563887085025734\n",
      "Epoch 30, Train Loss: 0.04496587174372145, Validation Loss: 0.04477887354038318\n",
      "Epoch 31, Train Loss: 0.04410761215098027, Validation Loss: 0.04397315226287584\n",
      "Epoch 32, Train Loss: 0.0432967359042599, Validation Loss: 0.04324803522135738\n",
      "Epoch 33, Train Loss: 0.04253652211483292, Validation Loss: 0.042520466043195246\n",
      "Epoch 34, Train Loss: 0.04180989392530481, Validation Loss: 0.0418886664722647\n",
      "Epoch 35, Train Loss: 0.04112270134221031, Validation Loss: 0.04126261851591308\n",
      "Epoch 36, Train Loss: 0.04046715149832946, Validation Loss: 0.04064619772214092\n",
      "Epoch 37, Train Loss: 0.03983624385379144, Validation Loss: 0.040131845696462574\n",
      "Epoch 38, Train Loss: 0.03924003469984274, Validation Loss: 0.03953665544243225\n",
      "Epoch 39, Train Loss: 0.03867028410032509, Validation Loss: 0.03897337766059184\n",
      "Epoch 40, Train Loss: 0.03813017712810191, Validation Loss: 0.03847402734000508\n",
      "Epoch 41, Train Loss: 0.03761347595020908, Validation Loss: 0.038000272048267336\n",
      "Epoch 42, Train Loss: 0.03711004513490429, Validation Loss: 0.03756972247870957\n",
      "Epoch 43, Train Loss: 0.036631006278169086, Validation Loss: 0.03713012645702537\n",
      "Epoch 44, Train Loss: 0.0361749146948873, Validation Loss: 0.03668021756623473\n",
      "Epoch 45, Train Loss: 0.035738321475899004, Validation Loss: 0.03631438092954898\n",
      "Epoch 46, Train Loss: 0.035307099248600086, Validation Loss: 0.03591221018016131\n",
      "Epoch 47, Train Loss: 0.03489492279692588, Validation Loss: 0.035623251671419325\n",
      "Epoch 48, Train Loss: 0.03450116320937846, Validation Loss: 0.03519142489192378\n",
      "Epoch 49, Train Loss: 0.03412273499884082, Validation Loss: 0.034848060322607434\n",
      "Epoch 50, Train Loss: 0.033755216935897975, Validation Loss: 0.0345504120096097\n",
      "Epoch 51, Train Loss: 0.033398305531185196, Validation Loss: 0.03419811510925509\n",
      "Epoch 52, Train Loss: 0.03306054145604398, Validation Loss: 0.03389021655616029\n",
      "Epoch 53, Train Loss: 0.0327307733300336, Validation Loss: 0.03359941789030198\n",
      "Epoch 54, Train Loss: 0.03240341908820526, Validation Loss: 0.03333979908000509\n",
      "Epoch 55, Train Loss: 0.032089107547283384, Validation Loss: 0.03305591974498295\n",
      "Epoch 56, Train Loss: 0.03178369583368769, Validation Loss: 0.032744309264608376\n",
      "Epoch 57, Train Loss: 0.031489232496570374, Validation Loss: 0.03251628353802377\n",
      "Epoch 58, Train Loss: 0.031204662718521892, Validation Loss: 0.03228027900544608\n",
      "Epoch 59, Train Loss: 0.030932168736111994, Validation Loss: 0.032038695201641175\n",
      "Epoch 60, Train Loss: 0.030657847948638687, Validation Loss: 0.03177019405967267\n",
      "Epoch 61, Train Loss: 0.030397392562554002, Validation Loss: 0.03151613847070247\n",
      "Epoch 62, Train Loss: 0.03014208218074666, Validation Loss: 0.03131634911411938\n",
      "Epoch 63, Train Loss: 0.02989309081936023, Validation Loss: 0.031088481485220615\n",
      "Epoch 64, Train Loss: 0.029645632124248544, Validation Loss: 0.030857188075469347\n",
      "Epoch 65, Train Loss: 0.029410346385997696, Validation Loss: 0.030693163238595587\n",
      "Epoch 66, Train Loss: 0.029182382681469554, Validation Loss: 0.030494886538085207\n",
      "Epoch 67, Train Loss: 0.02895568659140971, Validation Loss: 0.030296228014789393\n",
      "Epoch 68, Train Loss: 0.02873100716955213, Validation Loss: 0.030056752793438966\n",
      "Epoch 69, Train Loss: 0.02852286577936012, Validation Loss: 0.02991861093283115\n",
      "Epoch 70, Train Loss: 0.028307241269118544, Validation Loss: 0.029701533139998074\n",
      "Epoch 71, Train Loss: 0.028106966319005958, Validation Loss: 0.029503149743019912\n",
      "Epoch 72, Train Loss: 0.027904364209761803, Validation Loss: 0.029401253383178328\n",
      "Epoch 73, Train Loss: 0.027705895522026364, Validation Loss: 0.02917227489737476\n",
      "Epoch 74, Train Loss: 0.027512172331226423, Validation Loss: 0.0290351057626333\n",
      "Epoch 75, Train Loss: 0.02732231088693935, Validation Loss: 0.028902323702382708\n",
      "Epoch 76, Train Loss: 0.027142918299243778, Validation Loss: 0.028785097101242493\n",
      "Epoch 77, Train Loss: 0.02696506952121529, Validation Loss: 0.02860715204220304\n",
      "Epoch 78, Train Loss: 0.02678330923627929, Validation Loss: 0.028463598947569675\n",
      "Epoch 79, Train Loss: 0.026611060828925375, Validation Loss: 0.028364272771941867\n",
      "Epoch 80, Train Loss: 0.026440020527206717, Validation Loss: 0.028118218343951562\n",
      "Epoch 81, Train Loss: 0.02627158478964461, Validation Loss: 0.02798411489272886\n",
      "Epoch 82, Train Loss: 0.02610781681809149, Validation Loss: 0.02784874307102027\n",
      "Epoch 83, Train Loss: 0.025952068551296552, Validation Loss: 0.027759899735736306\n",
      "Epoch 84, Train Loss: 0.025788792244986078, Validation Loss: 0.027569189985714306\n",
      "Epoch 85, Train Loss: 0.02563412948215174, Validation Loss: 0.02748513962580248\n",
      "Epoch 86, Train Loss: 0.025487201656162115, Validation Loss: 0.027333489482904354\n",
      "Epoch 87, Train Loss: 0.025334069412770017, Validation Loss: 0.027240296952322593\n",
      "Epoch 88, Train Loss: 0.02519363525007716, Validation Loss: 0.027108706829206245\n",
      "Epoch 89, Train Loss: 0.02504716980910057, Validation Loss: 0.02698360479563371\n",
      "Epoch 90, Train Loss: 0.024903037491942243, Validation Loss: 0.02685099849574761\n",
      "Epoch 91, Train Loss: 0.02475849357973573, Validation Loss: 0.02675855266196387\n",
      "Epoch 92, Train Loss: 0.024628466968737224, Validation Loss: 0.026632287844574203\n",
      "Epoch 93, Train Loss: 0.024493407237212765, Validation Loss: 0.026533631471599022\n",
      "Epoch 94, Train Loss: 0.024354625324399554, Validation Loss: 0.026416720819208473\n",
      "Epoch 95, Train Loss: 0.024222494666517404, Validation Loss: 0.0264459147737832\n",
      "Epoch 96, Train Loss: 0.02409783184184078, Validation Loss: 0.02627199765891875\n",
      "Epoch 97, Train Loss: 0.023974851571971836, Validation Loss: 0.026143274581520607\n",
      "Epoch 98, Train Loss: 0.02385198546097709, Validation Loss: 0.026060424271309002\n",
      "Epoch 99, Train Loss: 0.023728748864259606, Validation Loss: 0.025955333588612413\n",
      "Epoch 100, Train Loss: 0.023606155899945247, Validation Loss: 0.025846895180928166\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from nltk.corpus import brown\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load the dataset\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# Create vocabularies for words and tags\n",
    "word_counts = Counter(word.lower() for sentence in sentences for word, _ in sentence)\n",
    "tag_counts = Counter(tag for sentence in sentences for _, tag in sentence)\n",
    "\n",
    "word2idx = {word: i + 2 for i, (word, _) in enumerate(word_counts.items())}\n",
    "word2idx[\"<PAD>\"] = 0  # Padding token\n",
    "word2idx[\"<UNK>\"] = 1  # Unknown word token\n",
    "tag2idx = {tag: i for i, (tag, _) in enumerate(tag_counts.items())}\n",
    "\n",
    "# Function to load GloVe embeddings\n",
    "def load_glove_embeddings(path, word2idx, embedding_dim):\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        embeddings = np.zeros((len(word2idx), embedding_dim))\n",
    "        for line in f:\n",
    "            values = line.split()\n",
    "            word = values[0]\n",
    "            vector = np.asarray(values[1:], dtype='float32')\n",
    "            idx = word2idx.get(word)\n",
    "            if idx:\n",
    "                embeddings[idx] = vector\n",
    "        return torch.tensor(embeddings, dtype=torch.float)\n",
    "\n",
    "# Load GloVe embeddings\n",
    "embedding_dim = 200\n",
    "glove_embeddings = load_glove_embeddings('glove.6B.200d.txt', word2idx, embedding_dim)\n",
    "\n",
    "# Convert sentences to sequences of indices and truncate/pad sentences\n",
    "max_len = 100\n",
    "\n",
    "def pad_and_truncate(sentence, tag, max_len):\n",
    "    if len(sentence) > max_len:\n",
    "        return sentence[:max_len], tag[:max_len]\n",
    "    else:\n",
    "        padding_len = max_len - len(sentence)\n",
    "        return sentence + [word2idx[\"<PAD>\"]] * padding_len, tag + [0] * padding_len\n",
    "\n",
    "data = [pad_and_truncate(\n",
    "    [word2idx.get(word.lower(), word2idx[\"<UNK>\"]) for word, _ in sentence], \n",
    "    [tag2idx[tag] for _, tag in sentence], max_len) \n",
    "    for sentence in sentences]\n",
    "\n",
    "# Split the dataset into training and test sets\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dataset and DataLoader\n",
    "class POSDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return torch.tensor(self.data[idx][0], dtype=torch.long), torch.tensor(self.data[idx][1], dtype=torch.long)\n",
    "\n",
    "def collate_fn(batch):\n",
    "    sentences, tags = zip(*batch)\n",
    "    return torch.stack(sentences), torch.stack(tags)\n",
    "\n",
    "train_loader = DataLoader(POSDataset(train_data), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "test_loader = DataLoader(POSDataset(test_data), batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "# LSTM Tagger Model\n",
    "class LSTMTagger(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size, pretrained_embeddings):\n",
    "        super(LSTMTagger, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        self.word_embeddings = nn.Embedding.from_pretrained(pretrained_embeddings, freeze=False)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        tag_space = self.hidden2tag(lstm_out.reshape(-1, self.hidden_dim))\n",
    "        tag_scores = nn.functional.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "model = LSTMTagger(embedding_dim, 128, len(word2idx), len(tag2idx), glove_embeddings)\n",
    "\n",
    "# Early Stopping\n",
    "class EarlyStopping:\n",
    "    def __init__(self, patience=50):\n",
    "        self.patience = patience\n",
    "        self.counter = 0\n",
    "        self.best_loss = None\n",
    "        self.early_stop = False\n",
    "\n",
    "    def __call__(self, val_loss):\n",
    "        if self.best_loss is None:\n",
    "            self.best_loss = val_loss\n",
    "        elif val_loss > self.best_loss:\n",
    "            self.counter += 1\n",
    "            if self.counter >= self.patience:\n",
    "                self.early_stop = True\n",
    "        else:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "\n",
    "# Training Loop with Early Stopping\n",
    "loss_function = nn.NLLLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "train_loader = DataLoader(POSDataset(train_data), batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(POSDataset(val_data), batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "early_stopping = EarlyStopping(patience=50)\n",
    "\n",
    "for epoch in range(100):  # Max number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for sentences, tags in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        tag_scores = model(sentences)\n",
    "        loss = loss_function(tag_scores.view(-1, len(tag2idx)), tags.view(-1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # Validation loop\n",
    "    val_loss = 0\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for sentences, tags in val_loader:\n",
    "            tag_scores = model(sentences)\n",
    "            loss = loss_function(tag_scores.view(-1, len(tag2idx)), tags.view(-1))\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {total_loss / len(train_loader)}, Validation Loss: {val_loss / len(val_loader)}\")\n",
    "    \n",
    "    # Check early stopping condition\n",
    "    early_stopping(val_loss / len(val_loader))\n",
    "    if early_stopping.early_stop:\n",
    "        print(\"Early stopping triggered\")\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 99.12%\n"
     ]
    }
   ],
   "source": [
    "# Function to calculate accuracy\n",
    "def calculate_accuracy(true_tags, pred_tags):\n",
    "    correct = pred_tags.eq(true_tags).sum().item()\n",
    "    total = true_tags.numel()\n",
    "    return correct / total\n",
    "\n",
    "# Evaluate on test data\n",
    "model.eval()\n",
    "total_accuracy = 0\n",
    "with torch.no_grad():\n",
    "    for sentences, tags in test_loader:\n",
    "        tag_scores = model(sentences)\n",
    "        _, predicted_tags = torch.max(tag_scores, dim=1)\n",
    "        accuracy = calculate_accuracy(tags.view(-1), predicted_tags)\n",
    "        total_accuracy += accuracy\n",
    "\n",
    "# Calculate average accuracy\n",
    "average_accuracy = total_accuracy / len(test_loader)\n",
    "print(f\"Test Accuracy: {average_accuracy * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Improvement over baseline = 4.43%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.430000000000007"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "99.12-94.69"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8342749529190202\n"
     ]
    }
   ],
   "source": [
    "print(cohens_kappa(.9912, .9469))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This suggests almost perfect agreement "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part D:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Provide an analysis of what experiments you conducted with hyperparameters, what your results were, and in particular comment on how the two methods compare, especially given that one has *no* choice of hyperparameters, and one has *many* choices of parameters. How useful was the flexibility of choice in hyperparameters in Part C?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This code follows the above listed objectives for 3C \n",
    "\n",
    "Data Preparation: The Brown Corpus is loaded and processed, with sentences truncated or padded to 100 tokens.\n",
    "\n",
    "GloVe Embeddings: It incorporates GloVe embeddings of dimension 200, updated with the words from the Brown Corpus.\n",
    "\n",
    "LSTM Model: An LSTM model is defined with configurable parameters for hidden_size, bidirectional, num_layers, and dropout.\n",
    "\n",
    "Early Stopping Mechanism: Implemented with a patience of 50 epochs.\n",
    "\n",
    "Training Loop: Includes training and validation phases, with early stopping based on validation loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The various hyperparameters in LSTM POS tagging code include:\n",
    "\n",
    "Embedding Dimension (embedding_dim): Set to 200, this is the size of the GloVe word embeddings.\n",
    "\n",
    "Hidden Size (hidden_dim): Configured as 128, it's the size of the LSTM's hidden layers.\n",
    "\n",
    "Learning Rate: In the optimizer (SGD), the learning rate is set to 0.1.\n",
    "\n",
    "Batch Size: Used in the data loaders, set to 32, determining how many samples per batch to pass through the network.\n",
    "\n",
    "Maximum Sentence Length (max_len): Sentences are truncated or padded to a length of 100 tokens.\n",
    "\n",
    "Patience for Early Stopping: Set to 50, this determines the number of epochs to wait for improvement before stopping."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Embedding dimension was specified in the last and 200D glove embeddings are apt for the task "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Various options for Hidden size :  64, 128, 256, 512. \n",
    "i tried 64 and 128 and 128 worked better \n",
    "The \"hidden size\" in an LSTM model refers to the dimensionality of the hidden state of each LSTM cell\n",
    "The optimal hidden size depends on the complexity of the task and the amount of data:\n",
    "Small datasets: A smaller hidden size can prevent overfitting.\n",
    "Large datasets and complex tasks: A larger hidden size may capture more information and improve model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate in machine learning models is a crucial hyperparameter that influences how quickly a model learns. Common options for learning rates include:\n",
    "Standard Values: Often between 0.1 and 0.0001. Common choices are 0.1, 0.01, 0.001, 0.0001.\n",
    "The optimal learning rate depends on the specific dataset and model architecture. A rate too high can cause the model to converge too quickly to a suboptimal solution, while a rate too low can make the training process unnecessarily long or cause it to stall.\n",
    "\n",
    "I tried 0.01 but it was taking too long so i terminated it and ran with .1\n",
    "even this took 13 hours to finish running "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Common options for batch sizes in training neural networks include:\n",
    "Small Batch Sizes: Like 32, 64, or 128. Smaller batches often provide a regularizing effect and lower generalization error.\n",
    "Large Batch Sizes: Like 256, 512, 1024, or even higher. Larger batches allow for more efficient computation (due to better GPU utilization) but might lead to poorer generalization\n",
    "\n",
    "Since i was running on my CPU, i went with 32 and 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maximum Sentence Length : Prof specified on Piazza to take 100 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Patience for Early Stopping: Set to 50 as specified "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So the 4 models i tried were : hidden_dim 64 - batch size 32, hidden_dum 64 - batch size 64, hidden_dim 128 - batch size 32,hidden_dim 128 - batch size 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hidden Dimension (hidden_dim)\tBatch Size\tAccuracy (%)\n",
    "# 64\t32\t98.533\n",
    "# 64\t64\t98.754\n",
    "# 128\t32\t99.12\n",
    "# 128\t64\t98.9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the best model was hidden_dim 128 - batch size 32"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional:\n",
    "\n",
    "You might want to try doing this problem with a transformer model such as BERT. There are plenty of blog posts out there describing the details, and, as usual, chatGPT would have plenty of things to say about the topic.... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using BERT (Bidirectional Encoder Representations from Transformers) for POS tagging is an advanced approach that leverages deep learning and the power of pre-trained language models. The process typically involves:\n",
    "\n",
    "Selecting a Pre-trained BERT Model: Choose a BERT model pre-trained on a large corpus, like BERT-base or BERT-large.\n",
    "\n",
    "Fine-Tuning the Model: Fine-tune the BERT model on a POS tagging dataset. This involves adding a classification layer on top of BERT to predict POS tags.\n",
    "\n",
    "Data Preparation: Tokenize your data in a format compatible with BERT, which may involve special tokens like [CLS] and [SEP].\n",
    "\n",
    "Training and Evaluation: Train the model on your POS tagging data, then evaluate its performance, optimizing hyperparameters as necessary.\n",
    "\n",
    "Inference: Use the fine-tuned model to predict POS tags on new sentences.\n",
    "\n",
    "This approach benefits from BERT's deep contextualized word representations, potentially leading to superior POS tagging performance compared to traditional methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package brown to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package universal_tagset to\n",
      "[nltk_data]     /Users/gauravbindra/nltk_data...\n",
      "[nltk_data]   Package universal_tagset is already up-to-date!\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1b49ee00b4a44e1e8e175e81bdf6c825",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/lib/python3.11/site-packages/transformers/optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb Cell 91\u001b[0m line \u001b[0;36m8\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb#Y156sZmlsZQ%3D%3D?line=78'>79</a>\u001b[0m outputs \u001b[39m=\u001b[39m model(batch_input_ids, labels\u001b[39m=\u001b[39mbatch_labels)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb#Y156sZmlsZQ%3D%3D?line=79'>80</a>\u001b[0m loss \u001b[39m=\u001b[39m outputs\u001b[39m.\u001b[39mloss\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb#Y156sZmlsZQ%3D%3D?line=80'>81</a>\u001b[0m loss\u001b[39m.\u001b[39;49mbackward()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb#Y156sZmlsZQ%3D%3D?line=81'>82</a>\u001b[0m total_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/gauravbindra/Desktop/NLP/hw05/HW05_checkpoint1.ipynb#Y156sZmlsZQ%3D%3D?line=82'>83</a>\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[39mif\u001b[39;00m has_torch_function_unary(\u001b[39mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[39mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[39m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[39mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[39m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m torch\u001b[39m.\u001b[39;49mautograd\u001b[39m.\u001b[39;49mbackward(\n\u001b[1;32m    493\u001b[0m     \u001b[39mself\u001b[39;49m, gradient, retain_graph, create_graph, inputs\u001b[39m=\u001b[39;49minputs\n\u001b[1;32m    494\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.11/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[39m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[39m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[39m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[39m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m Variable\u001b[39m.\u001b[39;49m_execution_engine\u001b[39m.\u001b[39;49mrun_backward(  \u001b[39m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m     tensors,\n\u001b[1;32m    253\u001b[0m     grad_tensors_,\n\u001b[1;32m    254\u001b[0m     retain_graph,\n\u001b[1;32m    255\u001b[0m     create_graph,\n\u001b[1;32m    256\u001b[0m     inputs,\n\u001b[1;32m    257\u001b[0m     allow_unreachable\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    258\u001b[0m     accumulate_grad\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m    259\u001b[0m )\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizerFast, BertForTokenClassification, AdamW\n",
    "from nltk.corpus import brown\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import nltk\n",
    "\n",
    "# Load the dataset\n",
    "nltk.download('brown')\n",
    "nltk.download('universal_tagset')\n",
    "sentences = brown.tagged_sents(tagset='universal')\n",
    "\n",
    "# Create tag vocabulary\n",
    "tags = {tag for sentence in sentences for _, tag in sentence}\n",
    "tag2idx = {tag: idx for idx, tag in enumerate(tags)}\n",
    "idx2tag = {idx: tag for tag, idx in tag2idx.items()}\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Function to encode text and preserve labels\n",
    "def encode_with_labels(sentence, labels):\n",
    "    inputs = tokenizer(sentence, add_special_tokens=True, padding='max_length', truncation=True, max_length=128, return_tensors=\"pt\")\n",
    "    input_ids = inputs['input_ids'].squeeze(0)\n",
    "    label_ids = [-100] + labels[:126] + [-100]  # -100 is the ignore index in PyTorch\n",
    "    label_ids += [-100] * (128 - len(label_ids))  # Padding labels\n",
    "    return input_ids, label_ids\n",
    "\n",
    "# Encode each sentence and its tags\n",
    "encoded_texts_and_labels = [\n",
    "    encode_with_labels(\" \".join([word for word, _ in sent]), [tag2idx[tag] for _, tag in sent])\n",
    "    for sent in sentences\n",
    "]\n",
    "\n",
    "# Prepare dataset\n",
    "input_ids = torch.stack([x[0] for x in encoded_texts_and_labels])\n",
    "labels = torch.stack([torch.tensor(x[1]) for x in encoded_texts_and_labels])\n",
    "\n",
    "# Train/test split\n",
    "train_inputs, val_inputs, train_labels, val_labels = train_test_split(input_ids, labels, random_state=2018, test_size=0.1)\n",
    "\n",
    "# Create Datasets\n",
    "class POSDataset(Dataset):\n",
    "    def __init__(self, inputs, labels):\n",
    "        self.inputs = inputs\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.inputs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.inputs[idx], self.labels[idx]\n",
    "\n",
    "train_data = POSDataset(train_inputs, train_labels)\n",
    "val_data = POSDataset(val_inputs, val_labels)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_data, batch_size=32)\n",
    "\n",
    "# Initialize BERT model for token classification\n",
    "model = BertForTokenClassification.from_pretrained(\"bert-base-uncased\", num_labels=len(tag2idx))\n",
    "\n",
    "# Training settings\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=3e-5)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(3):  # Number of epochs\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch in train_loader:\n",
    "        batch_input_ids, batch_labels = batch\n",
    "        batch_input_ids = batch_input_ids.to(device)\n",
    "        batch_labels = batch_labels.to(device)\n",
    "        \n",
    "        model.zero_grad()\n",
    "        outputs = model(batch_input_ids, labels=batch_labels)\n",
    "        loss = outputs.loss\n",
    "        loss.backward()\n",
    "        total_loss += loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_loader)\n",
    "    print(f\"Epoch {epoch + 1}, Train Loss: {avg_train_loss}\")\n",
    "\n",
    "# Add your evaluation code here if needed\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Would have taken more than a day just for a few epochs so i stopped running after a couple of hours "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code follows key steps:\n",
    "\n",
    "Data Preparation: You load the Brown Corpus, create a tag vocabulary, and tokenize the sentences using BERT's tokenizer, ensuring labels are aligned with tokenized inputs.\n",
    "\n",
    "BERT Model Initialization: You use BertForTokenClassification from Hugging Face's Transformers library, specifying the number of labels for classification.\n",
    "\n",
    "Dataset and DataLoader: You create a custom dataset and dataloaders for both training and validation data.\n",
    "\n",
    "Training Loop: The model is trained for 3 epochs with the AdamW optimizer. Loss is calculated and used for backpropagation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEXT STEPS "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next steps would include implementing an evaluation loop on the validation data to measure the model's performance, and potentially fine-tuning hyperparameters or training for more epochs for improved results. Remember to handle special tokens and alignment issues between BERT's subword tokenization and the POS tags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
